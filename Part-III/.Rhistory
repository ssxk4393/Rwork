map2 <- ggmap(map1)
map3 <- map2 + geom_point(aes(x=lon,y=lat,colour=house,size=house),data=df)
map3 + geom_text(data=df, aes(x=lon+0.01, y=lat+0.18,label=region),size=3)
map3 <- map2 + geom_point(aes(x=lon,y=lat,color=house,size=house),data=df)
map3 + geom_text(data=df, aes(x=lon+0.01, y=lat+0.18,label=region),size=3)
university <- read.csv("C:/NCS/Rwork/Part-II/university.csv",header=T)
university # 구청명  LAT(경도)    LON(위도)
df <- round(data.frame( x = jitter(rep(-95.36, 25), amount = .3),
y = jitter(rep(29.76, 25), amount = .3) ), digits = 2)
df
map <- get_googlemap('houston', markers = df, path = df, scale = 2)
ggmap(map, extent = 'device')
str(university)
df <- data.frame(university$LAT, university$LON)
map <- map_googlemap('Seoul', marker=df, path = df, scale=2)
map <- get_googlemap('Seoul', marker=df, path = df, scale=2)
ggmap(map, extent = 'device')
df <- round(data.frame(university$LAT, university$LON), digits=2)
map <- get_googlemap('Seoul', marker=df, path = df, scale=2)
map <- get_googlemap('Seoul', markers=df, path = df, scale=2)
ggmap(map, extent = 'device')
df <- round(data.frame(university$LON, university$LAT), digits=2)
map <- get_googlemap('Seoul', markers=df, path = df, scale=2)
ggmap(map, extent = 'device')
pop <- read.csv("C:/NCS/Rwork/Part-II/population201506.csv",header=T)
pop
map1 <- get_map("Jeonju", zoom=7 ,  maptype='roadmap')
map2 <- ggmap(map1)
map2
pop
map2 + geom_point(aes(x=lon,y=lat,color=house,size=house),data=pop)
map3 <- map2 + geom_point(aes(x=lon,y=lat,color=house,size=house),data=df)
map3 + geom_text(data=df, aes(x=lon+0.01, y=lat+0.18,label=region),size=3)
str(pop)
pop
str_replace_all(pop$세대수, ',','')
pop$세대수 <- str_replace_all(pop$세대수, ',','')
as.numeric(pop$세대수)
pop$세대수 <- as.numeric(pop$세대수)
str(pop)
map2 + geom_point(aes(x=lon,y=lat,color=pop$세대수,size=pop$세대수),data=pop)
map3 <- map2 + geom_point(aes(x=lon,y=lat,color=pop$세대수,size=pop$세대수),data=df)
map3 + geom_text(data=df, aes(x=lon+0.01, y=lat+0.18,label=region),size=3)
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre1.8.0_111')
library(rJava) # 로딩
install.packages(c("KoNLP", "tm", "wordcloud"))
library(KoNLP)
library(tm)
library(wordcloud) # RColorBrewer()함수 제공
facebook <- file("C:/NCS/Rwork/Part-II/facebook_bigdata.txt", encoding="UTF-8")
str(facebook)
facebook_data <- readLines(facebook) # 줄 단위 데이터 생성
head(facebook_data) # 앞부분 6줄 보기 - 줄 단위 문장 확인
str(facebook_data) # chr [1:76]
facebook_corpus <- Corpus(VectorSource(facebook_data))
facebook_corpus
inspect(facebook_corpus) # 76개 자료집에 포함된 문자 수 제공
useSejongDic() # 세종 사전 불러오기
library(KoNLP) # 세종사전. 8만여개정도의 단어가 들어가 있음.
useSejongDic() # 세종 사전 불러오기
install.packages(c("KoNLP", "tm", "wordcloud"))
install.packages(c("KoNLP", "tm", "wordcloud"))
install.packages(c("KoNLP", "tm", "wordcloud"))
install.packages(c("KoNLP", "tm", "wordcloud"))
library(KoNLP) # 세종사전. 8만여개정도의 단어가 들어가 있음.
library(tm)     # 영문 텍스트 마이닝 관련 라이브러리
library(wordcloud) # RColorBrewer()함수 제공
facebook <- file("C:/NCS/Rwork/Part-II/facebook_bigdata.txt", encoding="UTF-8")
facebook_data <- readLines(facebook) # 줄 단위 데이터 생성
facebook_corpus <- Corpus(VectorSource(facebook_data))
useSejongDic() # 세종 사전 불러오기
useSejongDic() # 세종 사전 불러오기
extractNoun('우리나라 대한민국 나는 홍길동 입니다.')
exNouns <- function(x) { paste(extractNoun(as.character(x)), collapse=" ")}
facebook_nouns <- sapply(facebook_corpus, exNouns)
facebook_nouns[1] # 단어만 추출된 첫 줄 보기
facebook_corpus
inspect(facebook_corpus) # 76개 자료집에 포함된 문자 수 제공
myCorputfacebook <- Corpus(VectorSource(facebook_nouns))
myCorputfacebook # Content:  documents: 76
myCorputfacebook <- tm_map(myCorputfacebook, removePunctuation) # 문장부호 제거
myCorputfacebook <- tm_map(myCorputfacebook, removeNumbers) # 수치 제거
myCorputfacebook <- tm_map(myCorputfacebook, tolower) # 소문자 변경
myCorputfacebook <-tm_map(myCorputfacebook, removeWords, stopwords('english')) # 불용어제거
stopwords('english')
inspect(myCorputfacebook[1:5]) # 데이터 전처리 결과 확인
inspect(myCorputfacebook[1:5]) # 데이터 전처리 결과 확인
useSejongDic() # 세종 사전 불러오기
useSejongDic()
install.packages('curl')
library(curl)
useSejongDic() # 세종 사전 불러오기
mergeUserDic(data.frame(c("R 프로그래밍","페이스북","소셜네트워크"), c("ncn")))
inspect(myCorputfacebook[1:5]) # 데이터 전처리 결과 확인
myCorputfacebook_txt <- tm_map(myCorputfacebook, PlainTextDocument)
myCorputfacebook_txt
myCorputfacebook_txt <- TermDocumentMatrix(myCorputfacebook_txt, control=list(wordLengths=c(2,Inf)))
myCorputfacebook_txt
myCorputfacebook_txt <- TermDocumentMatrix(myCorputfacebook_txt, control=list(wordLengths=c(2,Inf)))
myCorputfacebook_txt
inspect(myCorputfacebook_txt)
myCorputfacebook_txt <- TermDocumentMatrix(myCorputfacebook_txt, control=list(wordLengths=c(2,Inf)))
install.packages("SnowballC")
library(SnowballC)
myCorputfacebook_txt <- tm_map(myCorputfacebook, PlainTextDocument)
inspect(myCorputfacebook_txt)
myCorputfacebook_txt
myCorputfacebook_txt <- TermDocumentMatrix(myCorputfacebook_txt, control=list(wordLengths=c(2,Inf)))
myCorputfacebook_txt
myCorputfacebook_txt <- TermDocumentMatrix(myCorputfacebook_txt, control=list(wordLengths=c(2,Inf)))
facebook_data <- readLines(facebook, ok = TRUE, warn = FALSE, encoding = "utf-8") # 줄 단위 데이터 생성
convFacebook <- iconv(facebook_data, to = "utf-8")
head(convFacebook) # 앞부분 6줄 보기 - 줄 단위 문장 확인
facebook_data <- readLines(facebook, ok = TRUE, warn = FALSE, encoding = "utf-8") # 줄 단위 데이터 생성
convFace <- iconv(facebook_data, to = "utf-8")
tweets <- (convFace[!is.na(convFace)])
head(tweets) # 앞부분 6줄 보기 - 줄 단위 문장 확인
str(tweets) # chr [1:76]
facebook <- file("C:/NCS/Rwork/Part-II/facebook_bigdata.txt", encoding="UTF-8")
facebook_data <- readLines(facebook) # 줄 단위 데이터 생성
facebook_corpus <- Corpus(VectorSource(facebook_data))
facebook_corpus
inspect(facebook_corpus) # 76개 자료집에 포함된 문자 수 제공
mergeUserDic(data.frame(c("R 프로그래밍","페이스북","소셜네트워크"), c("ncn")))
exNouns <- function(x) { paste(extractNoun(as.character(x)), collapse=" ")}
facebook_nouns <- sapply(facebook_corpus, exNouns)
facebook_nouns[1] # 단어만 추출된 첫 줄 보기
facebook_nouns[1] # 단어만 추출된 첫 줄 보기
myCorputfacebook <- Corpus(VectorSource(facebook_nouns))
myCorputfacebook # Content:  documents: 76
myCorputfacebook # Content:  documents: 76
myCorputfacebook <- tm_map(myCorputfacebook, removePunctuation) # 문장부호 제거
myCorputfacebook <- tm_map(myCorputfacebook, removeNumbers) # 수치 제거
myCorputfacebook <- tm_map(myCorputfacebook, tolower) # 소문자 변경
myCorputfacebook <- tm_map(myCorputfacebook, removeWords, stopwords('english')) # 불용어제거
inspect(myCorputfacebook[1:5]) # 데이터 전처리 결과 확인
myCorputfacebook_txt <- tm_map(myCorputfacebook, PlainTextDocument)
myCorputfacebook_txt
myCorputfacebook_txt <- TermDocumentMatrix(myCorputfacebook_txt$content, control=list(wordLengths=c(2,Inf)))
myCorputfacebook_txt
myCorputfacebook_txt <- TermDocumentMatrix(myCorputfacebook_txt$meta, control=list(wordLengths=c(2,Inf)))
myCorputfacebook_txt <- TermDocumentMatrix(myCorputfacebook_txt, control=list(wordLengths=c(2,Inf)))
facebook_data
facebook_corpus <- Corpus(VectorSource(facebook_data))
facebook_corpus
inspect(facebook_corpus) # 76개 자료집에 포함된 문자 수 제공
facebook_corpus
str(facebook_data) # chr [1:76]
facebook_data
facebook[1]
facebook[[1]]
facebook_data[1]
facebook_corpus[1]
facebook_data
facebook_corpus <- Corpus(VectorSource(facebook_data))
facebook_corpus
inspect(facebook_corpus) # 76개 자료집에 포함된 문자 수 제공
facebook_corpus
exNouns <- function(x) { paste(extractNoun(as.character(x)), collapse=" ")}
facebook_nouns <- sapply(facebook_corpus, exNouns)
facebook_nouns[1] # 단어만 추출된 첫 줄 보기
myCorputfacebook_txt <- tm_map(myCorputfacebook, PlainTextDocument)
myCorputfacebook_txt
myCorputfacebook_txt <- TermDocumentMatrix(myCorputfacebook_txt, control=list(wordLengths=c(2,Inf)))
myCorputfacebook_txt
library(SnowballC)
myCorputfacebook_txt <- tm_map(myCorputfacebook,stemDocument) #평서문으로 변경
myCorputfacebook_txt <- PlainTextDocument(myCorputfacebook)
myCorputfacebook_txt
myCorputfacebook_txt <- TermDocumentMatrix(myCorputfacebook_txt, control=list(wordLengths=c(2,Inf)))
myCorputfacebook_txt
install.packages("http://cran.r-project.org/bin/windows/contrib/3.0/tm_0.5-10.zip",repos=NULL)
library(tm)
x <- rnorm(1000, 100, 10)
head(x)
x <- rnorm(1000, mean=100, sd=10)
head(x)
length(x)
hist(x)
shapiro.test(x)
t.test(x, mu = 95)
t.test(x, mu = 99)
t.test(x, mu = 95)
t.test(x, mu = 98)
t.test(x, mu = 99)
t.test(x, mu = 98)
t.test(x, mu = 95)
head(x)
hist(x)
t.test(x, mu=99.57)
t.test(x, mu=99.57, conf.level = 0.99)  # 99%
getwd()
setwd("c:/NCS/Rwork/Part-III")
data <- read.csv("one_sample.csv", header=TRUE)
head(data)
x <- data$survey
x
summary(x) # 결측치 확인
length(x) # 150개
table(x) # 0:불만족(14), 1: 만족(136)
x <- c(x,na)
summary(x) # 결측치 확인
data <- read.csv("one_sample.csv", header=TRUE)
head(data)
x <- data$survey
summary(x) # 결측치 확인
length(x) # 150개
table(x) # 0:불만족(14), 1: 만족(136)
library(prettyR) # freq() 함수 사용
freq(x)
binom.test(c(14,136), p=0.2) # 기존 20% 불만족율 기준 검증 실시
binom.test(c(14,136), p=0.2, alternative="two.sided", conf.level=0.95)
?binom.test
binom.test(14,136,p=0.2)
binom.test(14,150,p=0.2)
binom.test(c(14,136), p=0.2) # 기존 20% 불만족율 기준 검증 실시
binom.test(c(14,136), p=0.2, alternative="greater", conf.level=0.95)
binom.test(c(14,136), p=0.2) # 기존 20% 불만족율 기준 검증 실시
binom.test(c(14,136), p=0.2, alternative="greater", conf.level=0.95)
binom.test(c(14,136), p=0.2) # 기존 20% 불만족율 기준 검증 실시
binom.test(c(14,136), p=0.2, alternative="greater", conf.level=0.95)
binom.test(c(14,136), p=0.2, alternative="less", conf.level=0.95)
data <- read.csv("one_sample.csv", header=TRUE)
str(data) # 150
head(data)
x <- data$time
head(x)
summary(x) # NA-41개
mean(x) # NA
mean(x, na.rm=T) # NA 제외 평균(방법1)
x1 <- na.omit(x) # NA 제외 평균(방법2)
mean(x1)
shapiro.test(x1) # 정규분포 검정 함수(p-value = 0.7242)
shapiro.test(x1) # 정규분포 검정 함수(p-value = 0.7242)
t.test(x1, mu=5.2)
t.test(x1, mu=5.2, alter="two.side", conf.level=0.95) # p-value = 0.0001417
shapiro.test(x1) # 정규분포 검정 함수(p-value = 0.7242)
mean(x1)
head(x)
str(data) # 150
x <- data$time
mean(x1)
shapiro.test(x1) # 정규분포 검정 함수(p-value = 0.7242)
t.test(x1, mu=5.2)
t.test(x1, mu=5.2, alter="greater", conf.level=0.95)
t.test(x1, mu=5.2, alter="less", conf.level=0.95)
t.test(x1, mu=5.2, alter="greater", conf.level=0.95)
t.test(x1, mu=5.2, alter="greater", conf.level=0.95)
alter
t.test(x1, mu=5.2, alter="less", conf.level=0.95)
t.test(x, mu=99.57, conf.level = 0.99)  # 99%
binom.test(c(14,136), p=0.2, alternative="greater", conf.level=0.95)
binom.test(c(14,136), p=0.2, alternative="two.sided", conf.level=0.95)
binom.test(c(14,136), p=0.2, alternative="greater", conf.level=0.95)
binom.test(c(14,136), p=0.2, alternative="less", conf.level=0.95)
binom.test(c(14,136), p=0.2, alternative="greater", conf.level=0.95)
?binom.test
binom.test(c(14,136), p=0.2, alternative="greater", conf.level=0.95)
binom.test(c(14,136), p=0.2, alternative="greater", conf.level=0.95)
binom.test(c(14,136), p=0.2, alternative="less", conf.level=0.95)
?t.test
t.test(x1, mu=5.2, alter="greater", conf.level=0.95)
binom.test(c(14,136), p=0.2, alternative="greater", conf.level=0.95)
t.test(x1, mu=5.2, alter="greater", conf.level=0.95)
t.test(x1, mu=5.2, alter="greater", conf.level=0.95)
binom.test(c(14,136), p=0.2, alternative="greater", conf.level=0.95)
binom.test(c(14,136), p=0.2, alternative="greater", conf.level=0.95)
setwd("c:/NCS/Rwrok/Part-III")
data <- read.csv("one_sample.csv", header=TRUE)
setwd("c:/NCS/Rwork/Part-III")
data <- read.csv("one_sample.csv", header=TRUE)
str(data) # 150
head(data)
setwd("c:/NCS/Rwork/Part-III")
data <- read.csv("two_sample.csv", header=TRUE)
data
head(data) # 변수명 확인
data$method # 1, 2 -> 노이즈 없음
data$survey # 1(만족), 0(불만족)
x<- data$method # 교육방법(1, 2) -> 노이즈 없음
y<- data$survey # 만족도(1: 만족, 0:불만족)
x;y
table(x) # 1 : 150, 2 : 150
table(y) # 0 : 55, 1 : 245
head(data) # 변수명 확인
table(x) # 1 : 150, 2 : 150
table(x, y, useNA="ifany")
?prop.test
prop.test(c(110,135), c(150, 150)) # 14와 20% 불만족율 기준 차이 검정
prop.test(c(110,135), c(150, 150), alternative="two.sided", conf.level=0.95)
table(x, y, useNA="ifany")
prop.test(c(110,135), c(150, 150), alternative="greater", conf.level=0.95)
table(x, y, useNA="ifany")
prop.test(c(110,135), c(150, 150), alternative="two.sided", conf.level=0.95)
110/150
prop.test(c(110,135), c(150, 150), alternative="greater", conf.level=0.95)
prop.test(c(110,135), c(150, 150), alternative="greater", conf.level=0.95)
prop.test(c(110,135), c(150, 150), alternative="less", conf.level=0.95)
setwd("c:/NCS/Rwork/Part-III")
setwd("c:/NCS/Rwork/Part-III")
hdtv <- read.csv("hdtv.csv", header=TRUE)
str(hdtv)
hdtv
table(hdtv)
x <- hdtv$user.id
y <- hdtv$buy
table(y)
table(hdtv$buy)
x <- hdtv$buy
table(x)
length(x)
binom.test(c(10,40),p=0.15)
binom.test(c(10,40), p=0.15, alternative ='greater' )
setwd("c:/NCS/Rwork/Part-III")
height<- read.csv("student_height.csv", header=TRUE)
height<- read.csv("student_height.csv", header=TRUE)
str(height)
h <- height$height
mean(h)
summary(h)
shapiro.test(h)
wilcox.test(h)
wilcox.test(h, mu = 148.5)
wilcox.test(h, mu = 148.5, alternative = 'greater')
wilcox.test(h, mu = 148.5)
wilcox.test(h, mu = 148.5, alternative = 'greater')
wilcox.test(h, mu = 148.5, alternative = 'less')
wilcox.test(h, mu = 148.5, alternative = 'greater')
wilcox.test(h, mu = 148.5)
mean(h)
wilcox.test(h, mu = 148.5,conf.int = T)
table(x)
?binom.test
prop.test(c(110,135), c(150, 150), alternative="greater", conf.level=0.95)
prop.test(c(110,135), c(150, 150), alternative="greater", conf.level=0.95)
prop.test(c(110,135), c(150, 150), alternative="less", conf.level=0.95)
binom.test(c(14,136), p=0.2, alternative="greater", conf.level=0.95)
data <- read.csv("c:/NCS/Rwork/Part-III/two_sample.csv", header=TRUE)
data
head(data) #4개 변수 확인
summary(data) # score - NA's : 73개
result <- subset(data, !is.na(score), c(method, score))
dataset <- data[c('method', 'score')]
table(dataset$method)
head(dataset)
summary(dataset)
result <- subset(data, !is.na(score), c(method, score))
dataset <- result[c('method', 'score')]
head(dataset)
summary(dataset)
table(dataset$method)
method1 <- subset(dataset, method==1)
method2 <- subset(dataset, method==2)
method1_score <- method1$score
method2_score <- method2$score
length(method1_score); # 150
length(method2_score); # 150
var.test(method1_score, method2_score) # p-value = 0.3002 -> 차이가 없다.
t.test(method1_score, method2_score)
t.test(method1_score, method2_score, alter="two.sided", conf.int=TRUE, conf.level=0.95)
t.test(method1_score, method2_score, alter="greater", conf.int=TRUE, conf.level=0.95)
t.test(method1_score, method2_score, alter="less", conf.int=TRUE, conf.level=0.95)
data <- read.csv("two_sample.csv", header=T)
data
summary(data)
g <- data$gender
s <- data$survey
table(g,s)
s[s==1] <- '만족'
s[s==0] <- '불만'
table(g,s)
prop.test(c(36,138), c(19,107), alternative = "two.sided", conf.level = T)
prop.test(c(36,19), c(174,126), alternative = "two.sided", conf.level = T)
prop.test(c(36,19), c(174,126), alternative = "two.sided")
prop.test(c(36,19), c(174,126), alternative = "two.sided", conf.level = 0.95)
two <- read.csv('twomethod.csv', header = T)
str(two)
mothod1 <- two$method[two$method==1]
range(two$method)
summary(two)
sub <- subset[two, !is.na(score), c('method', 'score')]
sub <- subset(two, !is.na(score), c('method', 'score'))
sub
method1 <- subset(sub, method==1)
method2 <- subset(sub, method==2)
var.test(method1, method2)
unlist(method1)
unlist(method2)
var.test(method1, method2)
two <- read.csv('twomethod.csv', header = T)
str(two)
summary(two)
sub <- subset(two, !is.na(score), c('method', 'score'))
method1 <- subset(sub, method==1)
method2 <- subset(sub, method==2)
score1 <- subset(sub, method==1, score)
score2 <- subset(sub, method==2, score)
var.test(score1, score2)
class(score1)
score1
method1_score
as.vector(score1)
as.vector(score2)
var.test(score1, score2)
class(score1)
score1 <- as.vector(score1)
class(score1)
score1 <- unlist(score1)
class(score1)
score2 <- unlist(score2)
var.test(score1, score2)
t.test(score1, score2)
t.test(method1_score, method2_score)
str(two)
t.test(score1, score2)
t.test(method1_score, method2_score)
summary(dataset)
head(data) #4개 변수 확인
t.test(score1, score2)
getwd()
data <- read.csv("paired_sample.csv", header=TRUE)
result <- subset(data, !is.na(after), c(before,after))
dataset <- result[ c('before',  'after')]
dataset
before <- dataset$before# 교수법 적용전 점수
after <- dataset$after # 교수법 적용후 점수
before; after
length(before) # 100
length(after) # 100
summary(data)
mean(before) # 5.145
mean(after, na.rm = T) # 6.220833 -> 1.052  정도 증가
mean(after)
var.test(before, after, paired=TRUE)
t.test(before, after, paired=TRUE) # p-value < 2.2e-16
t.test(before, after, paired=TRUE,alter="greater",conf.int=TRUE, conf.level=0.95)
t.test(before, after, paired=TRUE,alter="less",conf.int=TRUE, conf.level=0.95)
summary(data)
setwd("c:/NCS/Rwork/Part-III")
data <- read.csv("three_sample.csv", header=TRUE)
data
str(data)
method <- data$method
survey<- data$survey
method
survey
table(method, useNA="ifany") # 50 50 50 -> 3그룹 모두 관찰치 50개
table(method, survey, useNA="ifany") # 그룹별 클릭수 : 1-43, 2-34, 3-37
table(method, useNA="ifany") # 50 50 50 -> 3그룹 모두 관찰치 50개
table(method, survey, useNA="ifany") # 그룹별 클릭수 : 1-43, 2-34, 3-37
prop.test(c(34,37,39), c(50,50,50)) # p-value = 0.1165 -> 귀무가설 채택
name(iris)
names(iris)
aov(Sepal.Length ~ Species, data=iris)
bartlett.test(Sepal.Length ~ Species, data=iris)
bartlett.test(Sepal.Width ~ Species, data=iris)
bartlett.test(Sepal.Width ~ Species, data=iris)
aov(Sepal.Width ~ Species, data=iris)
bartlett.test(Sepal.Width ~ Species, data=iris)
bartlett.test(Sepal.Width ~ Species, data=iris)
aov(Sepal.Width ~ Species, data=iris)
model <- aov(Sepal.Width ~ Species, data=iris)
summary(model)
bartlett.test(Sepal.Length ~ Species, data=iris)
kruskal.test(Sepal.Width ~ Species, data=iris)
aov(Sepal.Width ~ Species, data=iris)
summary(model)
model$residuals     # 잔차 = 관측치 - 예측치
model$fitted.values
summary(model)
TukeyHSD(model)
summary(kruskal.test(Sepal.Width ~ Species, data=iris))
model$fitted.values # 예측치
x <- rnorm(1000, mean=100, sd=10)
head(x)
hist(x)
shapiro.test(x)
mu <- mean(x)
z = (x-mu)/sd(x)
mean(z); sd(z)
z2 <- scale(x)
z2
mean(z2)
mean(z2); sd(z)
cost <- c(1.5, 5.1, 4.2, 3.5, 4.7, 3.5, 123)
range(cost)
log(cost)
log(cost+1)
log_cost <- log(cost+1)
range(log_cost)
non  <- function(x){
n <- ((x -min(x))/(max(x)-min(x)))
return(n)
}
nor  <- function(x){
n <- ((x -min(x))/(max(x)-min(x)))
return(n)
}
rm(non)
summary(iris[1:4])
apply(iris[1:4], nor)
result <- apply(iris[1:4], 2, nor)
summary(result)
summary(iris[1:4])
summary(result)
model$fitted.values # 예측치
bartlett.test(Sepal.Width ~ Species, data=iris)
summary(model)  # <2e-16 ***    세 집단은 차이를 보인다!
TukeyHSD(model)
model$fitted.values # 예측치
TukeyHSD(model)
plot(TukeyHSD(model))
