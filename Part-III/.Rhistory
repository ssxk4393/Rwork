plot(en$values, type="o") #
result <- factanal(drinking_water_df, factors = 2, rotation = 'varimax', scores = "regression")
result <- factanal(drinking_water_exam_df, factors = 2, rotation = 'varimax', scores = "regression")
result <- factanal(drinkig_water_exam_df, factors = 2, rotation = 'varimax', scores = "regression")
result # p-value is 9.87e-29 ===> 요인수 2개가 문제가 있다!
result
print(result, digits = 2, cutoff=0.5)
result$loadings
colnames(result$loadings) <- c('요인1','요인2')
result
head(result$scores)
plot(result$scores[,1], result$scores[,2], main='요인 1,2 시각화')
text(result$scores[,1], result$scores[,2],
labels = rownames(result$scores), cex = 0.7, pos = 3, col = "blue")
points(result$loadings[,c(1:2)], pch=19, col = "red")
text(result$loadings[,1], result$loadings[,2],
labels = rownames(result$loadings),
cex = 0.8, pos = 3, col = "red")
final <- as.data.frame(as.matrix(drinking_water_df) %*% as.matrix(result$loadings))
drinking_water_df
final <- as.data.frame(as.matrix(drinkig_water_exam_df) %*% as.matrix(result$loadings))
final
library(car)
fit <- lm(formula=Sepal.Length ~ Sepal.Width+Petal.Length+Petal.Width, data=iris)
vif(fit)
sqrt(vif(fit))>2 # root(VIF)가 2 이상인 것은 다중공선성 문제 의심
cor(iris[,-5]) # 변수간의 상관계수 보기(Species 제외)
x <- sample(1:nrow(iris), 0.7*nrow(iris)) # 전체중 70%만 추출
x
train <- iris[x, ] # 학습데이터 추출
test <- iris[-x, ] # 검정데이터 추출
result.lm <- lm(formula=Sepal.Length ~ Sepal.Width + Petal.Length, data=train)
result.lm
summary(result.lm)
summary(result.lm)
pred <- predict(result.lm, test)# x변수만 test에서 찾아서 값 예측
pred # test 데이터 셋의 y 예측치(회귀방정식 적용)
test$Sepal.Length # test 데이터 셋의 y 관측치
length(pred) # 45개 벡터
pred # test 데이터 셋의 y 예측치(회귀방정식 적용)
cor(pred, test$Sepal.Length)
cor(pred, test$Sepal.Length)^2
summary(pred); summary(test$Sepal.Length)
names(iris)
formula = Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width
f = Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width
rm(formula)
model <- lm(formula = f,  data=iris)
model
attributes(model)
model$coefficients
model$residuals
attributes(model)
step(model, direction="both")
step(model, direction="forward")
step(model, direction="backward")
plot(model)
methods(plot)
plot(model)
dev.off()
rm(list=ls())
f = Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width
model <- lm(formula = f,  data=iris)
methods(plot)
plot(model)
plot(model, which =  1)
attributes(model) # coefficients(계수), residuals(잔차), fitted.values(적합값)
res <- residuals(model) # 잔차 추출
model$residuals
res
res2 <- model$residuals
shapiro.test(res) # 정규성 검정 - p-value = 0.9349 >= 0.05
hist(res, freq = F)
qqnorm(res)
library(lmtest) # 자기상관 진단 패키지 설치
install.packages('lmtest')
library(lmtest) # 자기상관 진단 패키지 설치
dwtest(model) # 더빈 왓슨 값(통상 1~3 사이)
library(car)
sqrt(vif(model)) > 2 # TRUE
vif(model)
formula = Sepal.Length ~ Sepal.Width + Petal.Length
model <- lm(formula = formula,  data=iris)
summary(model) # 모델 평가
library(ggplot2)
data(diamonds)
str(diamonds)
lm(price~ ., data=diamonds)
result <- lm(price~ ., data=diamonds)
result
summary(result)
(diamonds)
str(diamonds)
result
diamonds
str(diamonds)
result$residuals
result <- lm(price~ carat + table+ depth, data=diamonds)
summary(result)
result
vif(result)
dwtest(result)
plot(result)
shapiro.test(result$residuals)
result$residuals
shapiro.test(result$residuals)
dwtest(result)
cor(result$coefficients)
cor(result)
cor(result$residuals, diamonds$price)
cor(result$fitted.values, diamonds$price)
summary(result)
summary(result)
shapiro.test(result$residuals)
str(diamonds)
sample(result, 5000)
sample(diamonds, 5000)
shapiro.test(sample(result$residuals, 5000))
dwtest(result)
dwtest(result)
dwtest(model) # 더빈 왓슨 값(통상 1~3 사이)
dwtest(result)
dev.off()
rm(list=ls())
weather = read.csv("c:/NCS/Rwork/Part-IV/weather.csv", stringsAsFactors = F)
weather = read.csv("c:/NCS/Rwork/Part-IV/weather.csv", stringsAsFactors = F)
dim(weather)  # 366  15
head(weather)
str(weather)
head(weather, 20)
str(weather)
weather_df <- weather[, c(-1, -6, -8, -14)]
str(weather_df)
weather_df$RainTomorrow[weather_df$RainTomorrow=='Yes'] <- 1
weather_df$RainTomorrow[weather_df$RainTomorrow=='No'] <- 0
weather_df$RainTomorrow <- as.numeric(weather_df$RainTomorrow)
head(weather_df)
idx <- sample(1:nrow(weather_df), nrow(weather_df)*0.7)
train <- weather_df[idx, ]
test <- weather_df[-idx, ]
weater_model <- glm(RainTomorrow ~ ., data = train, family = 'binomial')
weater_model
summary(weater_model)
str(weather)
summary(weater_model)
pred <- predict(weater_model, newdata=test, type="response")
pred
library(ROCR)
install.packages("ROCR")
library(ROCR)
pred
pr <- prediction(pred, test$RainTomorrow)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)
range(pred)
range(pred, na.rm = T)
summary(pred)
pred
sum(pred)
sum(pred, na.rm = T)
str(pred)
pred
ifelse(pred >= 0.5, 1, 0)
cpred <- ifelse(pred >= 0.5, 1, 0)
cpred[1:5]
pred[1:5]
table(cpred, test$RainTomorrow)
(85+10)/nrow(test)
summary(weater_model)
summary(weater_model)
summary(weater_model)$Coefficients
summary(weater_model)
weater_model
weater_model$coefficients
summary(weater_model)
weater_model$coefficients
summary(weater_model)
weater_model$coefficients
exp(0.64322873)
exp(-0.186843)
exp(weater_model$coefficients)
install.packages("ResourceSelection")
libarary(ResourceSelection)
library(ResourceSelection)
hoslem.test(weater_model$y, weater_model$fitted.values)
weater_model$y
train
s1 <- c(1, 2, 1, 2, 3, 4, 2, 3, 4, 5)
s2 <- c(1, 3, 1, 2, 3, 4, 2, 4, 3, 4)
s3 <- c(2, 3, 2, 3, 2, 3, 5, 3, 4, 2)
s4 <- c(2, 4, 2, 3, 2, 3, 5, 3, 4, 1)
s5 <- c(4, 5, 4, 5, 2, 1, 5, 2, 4, 3)
s6 <- c(4, 3, 4, 4, 2, 1, 5, 2, 4, 2)
name <-1:10
subject <- data.frame(s1, s2, s3, s4, s5, s6)
pc <- prcomp(subject) # scale = TRUE
summary(pc)
cor(subject) # 6개 과목변수 간의 상관계수 보기
en <- eigen(cor(subject)) # $values : 고유값, $vectors : 고유벡터
names(en)
en$values # 결과치가 1이상이면 요인으로 본다.
plot(en$values, type="o") # 고유값을 이용한 시각화
cor(subject)
result <- factanal(subject, factors = 2, rotation = "varimax")
result  # p-value is 0.0232 < 0.05  ====> 요인의 갯수 2개가 적절하지 않다.
result <- factanal(subject, factors = 3, # 요인 개수 지정
rotation = "varimax", # 회전방법 지정("varimax", "promax", "none")
scores="regression") # 요인점수 계산 방법
result # p-value가 나오지 않는다는 것은 요인 3개로 설정한것이 적절하다.
library(party) # ctree() 제공
dev.off()
rm(list=rs())
rm(list=ls())
install.packages("party")
library(party) # ctree() 제공
library(datasets)
data
data()
str(airquality)
air_ctree <- ctree(Temp ~ Solar.R +  Wind + Ozone, data=airquality)
air_ctree
plot(air_ctree)
result <- subset(airquality$Ozone <= 37 & airquality$Wind >15.5)
result <- subset(airquality, airquality$Ozone <= 37 & airquality$Wind >15.5)
result
str(airquality)
result$Temp
summary(result$Temp)
result$Temp
nrow(result)
summary(airquality)
idx <- sample(1:nrow(iris), nrow(iris) * 0.7)
train <- iris[idx,]
test <- iris[-idx,]
formula <- Species ~ Sepal.Length+Sepal.Width+Petal.Length+Petal.Width
iris_ctree <- ctree(formula, data=train) # 학습데이터로 분류모델(tree) 생성
iris_ctree # Petal.Length,Petal.Width 중요변수
?ctree
plot(iris_ctree, type="simple")
plot(iris_ctree) # 의사결정트리 해석
plot(iris_ctree, type="simple")
idx <- sample(1:nrow(iris), nrow(iris) * 0.7)
train <- iris[idx,]
test <- iris[-idx,]
formula <- Species ~ Sepal.Length+Sepal.Width+Petal.Length+Petal.Width
iris_ctree <- ctree(formula, data=train) # 학습데이터로 분류모델(tree) 생성
iris_ctree # Petal.Length,Petal.Width 중요변수
plot(iris_ctree, type="simple")
plot(iris_ctree) # 의사결정트리 해석
plot(iris_ctree, type="simple")
plot(iris_ctree) # 의사결정트리 해석
nrow(subset(iris, iris$Petal.Length > 1.9 & iris$Petal.Width <= 1.7 & iris$Petal.Length > 4.6))
subset(iris, iris$Petal.Length > 1.9 & iris$Petal.Width <= 1.7 & iris$Petal.Length > 4.6)
subset(iris, iris$Petal.Length > 1.7 & iris$Petal.Width <= 1.7 & iris$Petal.Length > 4.6)
nrow(subset(iris, iris$Petal.Length > 1.7 & iris$Petal.Width <= 1.7 & iris$Petal.Length > 4.6))
nrow(subset(iris, iris$Petal.Length > 1.7 & iris$Petal.Width <= 1.7 & iris$Petal.Length > 4.6))
nrow(subset(train, train$Petal.Length > 1.7 & train$Petal.Width <= 1.7 & train$Petal.Length > 4.6))
nrow(subset(train, Petal.Length > 1.7 & Petal.Width <= 1.7 & Petal.Length > 4.6))
subset(train, Petal.Length > 1.7 & Petal.Width <= 1.7 & Petal.Length > 4.6)
pred <- predict(iris_ctree, test) # 45
pred # Y변수의 변수값으로 예측
table(pred, test$Species)
library(cvTools)
cross <- cvFolds(nrow(iris), K=3, R=2)
cross
str(cross) # 구조 보기
length(cross$which) # 150
dim(cross$subsets) # 150   2
cross$subsets
cross$which
R=1:2 # 2회 반복
K=1:3 # 3겹
CNT=0 # 카운터 변수 -> 1차 테스트
ACC <- numeric() # 분류정확도 저장 -> 2차 모델 생성
for(r in R){ # 2회
cat('\n R=',r, '\n')
for(k in K){ # 3회
datas_idx <- cross$subsets[cross$which==k, r]
cat('K=',k,'검정데이터 \n')
print(iris[datas_idx, ])  # 검정데이터 생성
test <- iris[datas_idx, ]
for(i in K[-k]){  # 6회(3*2) 반복
formula <- Species ~ .
datas_idx <- cross$subsets[cross$which==i, r]
cat('K=',i,'훈련데이터 \n') # 학습데이터 생성
print(iris[datas_idx, ])
train <- iris[datas_idx, ]
CNT <- CNT + 1
}
} # outer for K
} # outer for R
table(pred, test$Species)
CNT=1 # 카운터 변수 -> 1차 테스트
for(r in R){ # 2회
cat('\n R=',r, '\n')
for(k in K){ # 3회
datas_idx <- cross$subsets[cross$which==k, r]
cat('K=',k,'검정데이터 \n')
print(iris[datas_idx, ])  # 검정데이터 생성
test <- iris[datas_idx, ]
for(i in K[-k]){  # 6회(3*2) 반복
formula <- Species ~ .
datas_idx <- cross$subsets[cross$which==i, r]
cat('K=',i,'훈련데이터 \n') # 학습데이터 생성
print(iris[datas_idx, ])
train <- iris[datas_idx, ]
model <- ctree(formula, data=train)   # 모델 생성
pred <- predict(model, test)  # 예측치 생성
re <- table(pred, test$Species)  # 분류정확도(acc)
ACC[CNT] <- (re[1,1]+re[2,2]+re[3,3])/nrow(test)
CNT <- CNT + 1
}
} # outer for K
} # outer for R
ACC
CNT=0 # 카운터 변수 -> 1차 테스트
for(r in R){ # 2회
cat('\n R=',r, '\n')
for(k in K){ # 3회
datas_idx <- cross$subsets[cross$which==k, r]
cat('K=',k,'검정데이터 \n')
print(iris[datas_idx, ])  # 검정데이터 생성
test <- iris[datas_idx, ]
for(i in K[-k]){  # 6회(3*2) 반복
formula <- Species ~ .
datas_idx <- cross$subsets[cross$which==i, r]
cat('K=',i,'훈련데이터 \n') # 학습데이터 생성
print(iris[datas_idx, ])
train <- iris[datas_idx, ]
model <- ctree(formula, data=train)   # 모델 생성
pred <- predict(model, test)  # 예측치 생성
re <- table(pred, test$Species)  # 분류정확도(acc)
ACC[CNT] <- (re[1,1]+re[2,2]+re[3,3])/nrow(test)
CNT <- CNT + 1
}
} # outer for K
} # outer for R
ACC
CNT=1 # 카운터 변수 -> 1차 테스트
for(r in R){ # 2회
cat('\n R=',r, '\n')
for(k in K){ # 3회
datas_idx <- cross$subsets[cross$which==k, r]
cat('K=',k,'검정데이터 \n')
print(iris[datas_idx, ])  # 검정데이터 생성
test <- iris[datas_idx, ]
for(i in K[-k]){  # 6회(3*2) 반복
formula <- Species ~ .
datas_idx <- cross$subsets[cross$which==i, r]
cat('K=',i,'훈련데이터 \n') # 학습데이터 생성
print(iris[datas_idx, ])
train <- iris[datas_idx, ]
model <- ctree(formula, data=train)   # 모델 생성
pred <- predict(model, test)  # 예측치 생성
re <- table(pred, test$Species)  # 분류정확도(acc)
ACC[CNT] <- (re[1,1]+re[2,2]+re[3,3])/nrow(test)
CNT <- CNT + 1
}
} # outer for K
} # outer for R
CNT=1 # 카운터 변수 -> 1차 테스트
ACC <- numeric() # 분류정확도 저장 -> 2차 모델 생성
for(r in R){ # 2회
cat('\n R=',r, '\n')
for(k in K){ # 3회
datas_idx <- cross$subsets[cross$which==k, r]
cat('K=',k,'검정데이터 \n')
print(iris[datas_idx, ])  # 검정데이터 생성
test <- iris[datas_idx, ]
for(i in K[-k]){  # 6회(3*2) 반복
formula <- Species ~ .
datas_idx <- cross$subsets[cross$which==i, r]
cat('K=',i,'훈련데이터 \n') # 학습데이터 생성
print(iris[datas_idx, ])
train <- iris[datas_idx, ]
model <- ctree(formula, data=train)   # 모델 생성
pred <- predict(model, test)  # 예측치 생성
re <- table(pred, test$Species)  # 분류정확도(acc)
ACC[CNT] <- (re[1,1]+re[2,2]+re[3,3])/nrow(test)
CNT <- CNT + 1
}
} # outer for K
} # outer for R
CNT # 12
ACC
mean(ACC)
R=1:2 # 2회 반복
K=1:3 # 3겹
CNT=1 # 카운터 변수 -> 1차 테스트
ACC <- numeric() # 분류정확도 저장 -> 2차 모델 생성
for(r in R){ # 2회
cat('\n R=',r, '\n')
for(k in K){ # 3회
datas_idx <- cross$subsets[cross$which==k, r]
cat('K=',k,'검정데이터 \n')
print(iris[datas_idx, ])  # 검정데이터 생성
test <- iris[datas_idx, ]
formula <- Species ~ .
datas_idx <- cross$subsets[-cross$which==k, r]
cat('K=',i,'훈련데이터 \n') # 학습데이터 생성
print(iris[datas_idx, ])
train <- iris[datas_idx, ]
model <- ctree(formula, data=train)   # 모델 생성
pred <- predict(model, test)  # 예측치 생성
re <- table(pred, test$Species)  # 분류정확도(acc)
ACC[CNT] <- (re[1,1]+re[2,2]+re[3,3])/nrow(test)
CNT <- CNT + 1
} # outer for K
} # outer for R
ACC
R=1:2 # 2회 반복
K=1:3 # 3겹
CNT=1 # 카운터 변수 -> 1차 테스트
ACC <- numeric() # 분류정확도 저장 -> 2차 모델 생성
for(r in R){ # 2회
cat('\n R=',r, '\n')
for(k in K){ # 3회
datas_idx <- cross$subsets[cross$which==k, r]
cat('K=',k,'검정데이터 \n')
print(iris[datas_idx, ])  # 검정데이터 생성
test <- iris[datas_idx, ]
formula <- Species ~ .
datas_idx <- cross$subsets[-(cross$which==k), r]
cat('K=',i,'훈련데이터 \n') # 학습데이터 생성
print(iris[datas_idx, ])
train <- iris[datas_idx, ]
model <- ctree(formula, data=train)   # 모델 생성
pred <- predict(model, test)  # 예측치 생성
re <- table(pred, test$Species)  # 분류정확도(acc)
ACC[CNT] <- (re[1,1]+re[2,2]+re[3,3])/nrow(test)
CNT <- CNT + 1
} # outer for K
} # outer for R
ACC
mean(ACC)
library(ggplot2)
data(mpg)
str(mpg)
sample(mpg, nrow(mpg)*0.7)
train <- sample(mpg, nrow(mpg)*0.7)
test <- mpg[-train,]
rm(train)
idx <- sample(1:nrow(mpg), nrow(mpg) * 0.7)
train <- mpg[idx,]
test <- mpg[-idx,]
str(mpg)
f <- cty~displ + cyl + year
model <- ctree(f, data=train)
pred <- predict(model, test)
pred
mpg
pred
plot(pred)
plot(model)
plot(model)
summary(pred)
plot(model)
cor(pred, test$cty)
nrow(train)
nrow(test)
idx <- sample(1:nrow(mpg), nrow(mpg) * 0.7)
train <- mpg[idx,]
test <- mpg[-idx,]
model <- ctree(f, data=train)
pred <- predict(model, test)
cor(pred, test$cty)
plot(model)
model
for(r in R){ # 2회
cat('\n R=',r, '\n')
for(k in K){ # 3회
datas_idx <- cross$subsets[cross$which==k, r]
cat('K=',k,'검정데이터 \n')
print(iris[datas_idx, ])  # 검정데이터 생성
test <- iris[datas_idx, ]
formula <- Species ~ .
datas_idx <- cross$subsets[-(cross$which==k), r]
cat('K=',i,'훈련데이터 \n') # 학습데이터 생성
print(iris[datas_idx, ])
train <- iris[datas_idx, ]
model <- ctree(formula, data=train)   # 모델 생성
pred <- predict(model, test)  # 예측치 생성
re <- table(pred, test$Species)  # 분류정확도(acc)
ACC[CNT] <- (re[1,1]+re[2,2]+re[3,3])/nrow(test)
CNT <- CNT + 1
} # outer for K
} # outer for R
mean(ACC)
ACC
library(arules)
data("AdultUCI")
str(AdultUCI) # 'data.frame':	48842 obs. of  15 variables:
str(AdultUCI) # 'data.frame':	48842 obs. of  15 variables:
names(AdultUCI)
set.seed(1234) # 메모리에 시드 값 적용
choice <- sample(1:nrow(AdultUCI), 10000)
choice
adult.df <-  AdultUCI[choice, ]
str(adult.df) # ' # 'data.frame':	10000 obs. of  15 variables:
capital<- adult.df$`capital-gain`
hours<- adult.df$`hours-per-week`
education <- adult.df$`education-num`
race <- adult.df$race
age <- adult.df$age
income <- adult.df$income
adult_df <- data.frame(capital=capital, age=age, race=race, hours=hours, education=education, income=income)
str(adult_df) # 'data.frame':	10000 obs. of  6 variables:
formula <-  capital ~ income + education + hours + race + age
adult_ctree <- ctree(formula, data=adult_df)
adult_ctree # 가장 큰 영향을 미치는 변수 - income, education
plot(adult_ctree)
choice
