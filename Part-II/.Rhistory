myCorputfacebook_txt
myCorputfacebook_txt <- TermDocumentMatrix(myCorputfacebook_txt$content, control=list(wordLengths=c(2,Inf)))
myCorputfacebook_txt
myCorputfacebook_txt <- TermDocumentMatrix(myCorputfacebook_txt$meta, control=list(wordLengths=c(2,Inf)))
myCorputfacebook_txt <- TermDocumentMatrix(myCorputfacebook_txt, control=list(wordLengths=c(2,Inf)))
facebook_data
facebook_corpus <- Corpus(VectorSource(facebook_data))
facebook_corpus
inspect(facebook_corpus) # 76개 자료집에 포함된 문자 수 제공
facebook_corpus
str(facebook_data) # chr [1:76]
facebook_data
facebook[1]
facebook[[1]]
facebook_data[1]
facebook_corpus[1]
facebook_data
facebook_corpus <- Corpus(VectorSource(facebook_data))
facebook_corpus
inspect(facebook_corpus) # 76개 자료집에 포함된 문자 수 제공
facebook_corpus
exNouns <- function(x) { paste(extractNoun(as.character(x)), collapse=" ")}
facebook_nouns <- sapply(facebook_corpus, exNouns)
facebook_nouns[1] # 단어만 추출된 첫 줄 보기
myCorputfacebook_txt <- tm_map(myCorputfacebook, PlainTextDocument)
myCorputfacebook_txt
myCorputfacebook_txt <- TermDocumentMatrix(myCorputfacebook_txt, control=list(wordLengths=c(2,Inf)))
myCorputfacebook_txt
library(SnowballC)
myCorputfacebook_txt <- tm_map(myCorputfacebook,stemDocument) #평서문으로 변경
myCorputfacebook_txt <- PlainTextDocument(myCorputfacebook)
myCorputfacebook_txt
myCorputfacebook_txt <- TermDocumentMatrix(myCorputfacebook_txt, control=list(wordLengths=c(2,Inf)))
myCorputfacebook_txt
install.packages("http://cran.r-project.org/bin/windows/contrib/3.0/tm_0.5-10.zip",repos=NULL)
library(tm)
install.packages("http://cran.r-project.org/bin/windows/contrib/3.0/tm_0.5-10.zip",repos=NULL)
library(tm)
library(KoNLP) # 세종사전. 8만여개정도의 단어가 들어가 있음.
library(wordcloud) # RColorBrewer()함수 제공
library(rJava) # 로딩
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre1.8.0_111')
facebook <- file("C:/NCS/Rwork/Part-II/facebook_bigdata.txt", encoding="UTF-8")
facebook_data <- readLines(facebook) # 줄 단위 데이터 생성
facebook_data
head(facebook_data) # 앞부분 6줄 보기 - 줄 단위 문장 확인
facebook_corpus <- Corpus(VectorSource(facebook_data))
facebook_corpus
library(curl)
exNouns <- function(x) { paste(extractNoun(as.character(x)), collapse=" ")}
facebook_nouns <- sapply(facebook_corpus, exNouns)
facebook_nouns[1] # 단어만 추출된 첫 줄 보기
myCorputfacebook <- Corpus(VectorSource(facebook_nouns))
myCorputfacebook # Content:  documents: 76
myCorputfacebook <- tm_map(myCorputfacebook, removePunctuation) # 문장부호 제거
myCorputfacebook <- tm_map(myCorputfacebook, removeNumbers) # 수치 제거
myCorputfacebook <- tm_map(myCorputfacebook, tolower) # 소문자 변경
myCorputfacebook <- tm_map(myCorputfacebook, removeWords, stopwords('english')) # 불용어제거
inspect(myCorputfacebook[1:5]) # 데이터 전처리 결과 확인
library(SnowballC)
myCorputfacebook_txt <- tm_map(myCorputfacebook, PlainTextDocument)
myCorputfacebook_txt
myCorputfacebook_txt <- TermDocumentMatrix(myCorputfacebook_txt, control=list(wordLengths=c(2,Inf)))
myCorputfacebook_txt
myTermfacebook.df <- as.data.frame(as.matrix(myCorputfacebook_txt))
dim(myTermfacebook.df) # [1] 876  76
wordResult <- sort(rowSums(myTermfacebook.df), decreasing=TRUE) # 빈도수로 내림차순 정렬
wordResult[1:10]
myName <- names(wordResult) # 단어 이름 생성 -> 빈도수의 이름
wordcloud(myName, wordResult) # 단어구름 적성
myCorputfacebook <- tm_map(myCorputfacebook, removePunctuation) # 문장부호 제거
myCorputfacebook <- tm_map(myCorputfacebook, removeNumbers) # 수치 제거
myCorputfacebook <- tm_map(myCorputfacebook, tolower) # 소문자 변경
myStopwords = c(stopwords('english'), "사용", "하기");
myCorputfacebook = tm_map(myCorputfacebook, removeWords, myStopwords);
inspect(myCorputfacebook[1:5]) # 데이터 전처리 결과 확인
myCorputfacebook_txt <- tm_map(myCorputfacebook, PlainTextDocument)
myCorputfacebook_txt
myCorputfacebook_txt <- TermDocumentMatrix(myCorputfacebook_txt, control=list(wordLengths=c(2,Inf)))
myCorputfacebook_txt
myTermfacebook.df <- as.data.frame(as.matrix(myCorputfacebook_txt))
dim(myTermfacebook.df) # [1] 876  76
wordResult <- sort(rowSums(myTermfacebook.df), decreasing=TRUE) # 빈도수로 내림차순 정렬
wordResult[1:10]
myName <- names(wordResult) # 단어 이름 추출(빈도수 이름)
wordcloud(myName, wordResult) # 단어구름 시각화
word.df <- data.frame(word=myName, freq=wordResult)
str(word.df) # word, freq 변수
pal <- brewer.pal(12,"Paired") # 12가지 색상 pal <- brewer.pal(9,"Set1") # Set1~ Set3
windowsFonts(malgun=windowsFont("맑은 고딕"))  #windows
x11( ) # 별도의 창을 띄우는 함수
wordcloud(word.df$word, word.df$freq,
scale=c(5,1), min.freq=3, random.order=F,
rot.per=.1, colors=pal, family="malgun")
topWord <- head(sort(wordResult, decreasing=T), 10) # 상위 10개 토픽추출
pie(topWord, col=rainbow(10), radius=1) # 파이 차트-무지개색, 원크기
pct <- round(topWord/sum(topWord)*100, 1) # 백분율
names(topWord)
lab <- paste(names(topWord), "\n", pct, "%")
pie(topWord, main="SNS 빅데이터 관련 토픽분석",
col=rainbow(10), cex=0.8, labels=lab)
wordcloud(word.df$word, word.df$freq,
scale=c(5,1), min.freq=3, random.order=F,
rot.per=.1, colors=pal, family="malgun")
x11( ) # 별도의 창을 띄우는 함수
wordcloud(word.df$word, word.df$freq,
scale=c(5,1), min.freq=3, random.order=F,
rot.per=.1, colors=pal, family="malgun")
myCorputfacebook_txt
myTermfacebook.df <- as.data.frame(as.matrix(myCorputfacebook_txt))
dim(myTermfacebook.df) # [1] 876  76
wordResult <- sort(rowSums(myTermfacebook.df), decreasing=TRUE) # 빈도수로 내림차순 정렬
wordResult[1:10]
rowSums(myTermfacebook.df)
wordResult[1:10]
myCorputfacebook_txt <- tm_map(myCorputfacebook, PlainTextDocument)
myCorputfacebook_txt
myCorputfacebook_txt <- TermDocumentMatrix(myCorputfacebook_txt, control=list(wordLengths=c(2,Inf)))
myCorputfacebook_txt
myTermfacebook.df <- as.data.frame(as.matrix(myCorputfacebook_txt))
dim(myTermfacebook.df) # [1] 880  76
wordResult <- sort(rowSums(myTermfacebook.df), decreasing=TRUE) # 빈도수로 내림차순 정렬
wordResult[1:10]
facebook <- file("C:/NCS/Rwork/Part-II/facebook_bigdata.txt", encoding="UTF-8")
facebook_data <- readLines(facebook) # 줄 단위 데이터 생성
facebook_corpus <- Corpus(VectorSource(facebook_data))
inspect(facebook_corpus) # 76개 자료집에 포함된 문자 수 제공
facebook_nouns <- sapply(facebook_corpus, exNouns)
facebook_nouns[1] # 단어만 추출된 첫 줄 보기
myCorputfacebook <- Corpus(VectorSource(facebook_nouns))
myCorputfacebook <- tm_map(myCorputfacebook, removePunctuation) # 문장부호 제거
myCorputfacebook <- tm_map(myCorputfacebook, removeNumbers) # 수치 제거
myCorputfacebook <- tm_map(myCorputfacebook, tolower) # 소문자 변경
myCorputfacebook <- tm_map(myCorputfacebook, removeWords, stopwords('english')) # 불용어제거
myCorputfacebook_txt <- tm_map(myCorputfacebook, PlainTextDocument)
myCorputfacebook_txt
myCorputfacebook_txt <- TermDocumentMatrix(myCorputfacebook_txt, control=list(wordLengths=c(2,Inf)))
myCorputfacebook_txt
wordResult <- sort(rowSums(myTermfacebook.df), decreasing=TRUE) # 빈도수로 내림차순 정렬
wordResult[1:10]
myName <- names(wordResult) # 단어 이름 생성 -> 빈도수의 이름
wordcloud(myName, wordResult) # 단어구름 적성
myCorputfacebook <- tm_map(myCorputfacebook, removePunctuation) # 문장부호 제거
myCorputfacebook <- tm_map(myCorputfacebook, removeNumbers) # 수치 제거
myCorputfacebook <- tm_map(myCorputfacebook, tolower) # 소문자 변경
myStopwords = c(stopwords('english'), "사용", "하기");
myCorputfacebook = tm_map(myCorputfacebook, removeWords, myStopwords);
inspect(myCorputfacebook[1:5]) # 데이터 전처리 결과 확인
myCorputfacebook_txt <- tm_map(myCorputfacebook, PlainTextDocument)
myCorputfacebook_txt
myCorputfacebook_txt <- TermDocumentMatrix(myCorputfacebook_txt, control=list(wordLengths=c(2,Inf)))
myCorputfacebook_txt
myTermfacebook.df <- as.data.frame(as.matrix(myCorputfacebook_txt))
myTermfacebook.df <- as.data.frame(as.matrix(myCorputfacebook_txt))
dim(myTermfacebook.df) # [1] 876  76
wordResult[1:10]
myName <- names(wordResult) # 단어 이름 추출(빈도수 이름)
wordcloud(myName, wordResult) # 단어구름 시각화
word.df <- data.frame(word=myName, freq=wordResult)
str(word.df) # word, freq 변수
pal <- brewer.pal(12,"Paired") # 12가지 색상 pal <- brewer.pal(9,"Set1") # Set1~ Set3
windowsFonts(malgun=windowsFont("맑은 고딕"))  #windows
x11( ) # 별도의 창을 띄우는 함수
wordcloud(word.df$word, word.df$freq,
scale=c(5,1), min.freq=3, random.order=F,
rot.per=.1, colors=pal, family="malgun")
topWord <- head(sort(wordResult, decreasing=T), 10) # 상위 10개 토픽추출
pie(topWord, col=rainbow(10), radius=1) # 파이 차트-무지개색, 원크기
pct <- round(topWord/sum(topWord)*100, 1) # 백분율
lab <- paste(names(topWord), "\n", pct, "%")
pie(topWord, main="SNS 빅데이터 관련 토픽분석",
col=rainbow(10), cex=0.8, labels=lab)
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre1.8.0_111')
marketing <- file("C:/NCS/Rwork/Part-II/marketing.txt", encoding="UTF-8")
marketing2 <- readLines(marketing) # 줄 단위 데이터 생성
close(marketing)
head(marketing2) # 앞부분 6줄 보기 - 줄 단위 문장 확인
str(marketing2)
lword <- Map(extractNoun, marketing2)   ## => map은 f(함수)와 data(데이터)를 1:1로 매핑
length(lword) # [1] 472
lword <- unique(lword) # 중복제거1(전체 대상)
length(lword) # [1] 353(19개 제거)
lword <- sapply(lword, unique) # 중복제거2(줄 단위 대상)
length(lword) # [1] 352(1개 제거)
str(lword) # List of 353
class(lword) # 추출 단어 확인
filter1 <- function(x){
nchar(x) <= 4 && nchar(x) >= 2 && is.hangul(x)
}
filter2 <- function(x){
Filter(filter1, x)
}
lword <- sapply(lword, filter2)
lword # 추출 단어 확인(길이 1개 단어 삭제됨)
str(lword) # 추출 단어 확인(길이 1개 단어 삭제됨)
lword # 추출 단어 확인(길이 1개 단어 삭제됨)
lword[1:20] # 추출 단어 확인(길이 1개 단어 삭제됨)
install.packages("arules")
library(arules)
Adult
data(Adult)
Adult
wordtran <- as(lword, "transactions") # lword에 중복데이터가 있으면 error발생
wordtran
inspect(wordtran)
class(wordtran)
mode(wordtran)
inspect(wordtran)
wordtable <- crossTable(wordtran) # 교차표 작성
length(wordtable) # 5890329
str(wordtable) # [1:2427, 1:2427]
version# 5.단어 간 연관규칙 산출
tranrules <- apriori(wordtran, parameter=list(supp=0.25, conf=0.05))
inspect(tranrules) # 연관규칙 생성 결과(59개) 보기
inspect(tranrules) # 연관규칙 생성 결과(59개) 보기
rules <- labels(tranrules, ruleSep=" ")
rules
class(rules)
# 문자열로 묶인 연관단어를 행렬구조 변경
rules <- sapply(rules, strsplit, " ",  USE.NAMES=F)
rules
class(rules) # [1] "list"
# 행 단위로 묶어서 matrix로 반환
rulemat <- do.call("rbind", rules)
rulemat
class(rulemat)
# (2) 연관어 시각화를 위한 igraph 패키지 설치
install.packages("igraph") # graph.edgelist(), plot.igraph(), closeness() 함수 제공
library(igraph)
# (3) edgelist보기 - 연관단어를 정점 형태의 목록 제공
ruleg <- graph.edgelist(rulemat[c(12:59),], directed=F) # [1,]~[11,] "{}" 제외
help(graph.edgelist)
ruleg
# (4) edgelist 시각화
X11()
plot.igraph(ruleg, vertex.label=V(ruleg)$name,
vertex.label.cex=1.2, vertex.label.color='black',
vertex.size=20, vertex.color='green', vertex.frame.color='blue')
X11()
plot.igraph(ruleg, vertex.label=V(ruleg)$name,
vertex.label.cex=1.2, vertex.label.color='black',
vertex.size=20, vertex.color='green', vertex.frame.color='blue')
data<-read.csv(file.choose()) # file.choose() 파일 선택
head(data,2)
dim(data) # 100   2
str(data) # 변수명 : company, review <- 고객 인터뷰 내용
library(plyr) # laply()함수 제공
library(stringr) # str_split()함수 제공
length(posDic) # 2006
setwd("C:/NCS/Rwork/Part-II")
posDic <- readLines("posDic.txt")
negDic <- readLines("negDic.txt")
length(posDic) # 2006
length(negDic) # 4783
posDic.final <-c(posDic, 'victor')
negDic.final <-c(negDic, 'vanquished')
posDic.final;
?plyr
a <- list(a=1:5)
a
laply(a, function(x){
x*2
})
laply(a, function(x,y,z){
x+y+z
}, 2, 3)
sentimental = function(sentences, posDic, negDic){
scores = laply(sentences, function(sentence, posDic, negDic) {
sentence = gsub('[[:punct:]]', '', sentence) #문장부호 제거
sentence = gsub('[[:cntrl:]]', '', sentence) #특수문자 제거
sentence = gsub('\\d+', '', sentence) # 숫자 제거
sentence = tolower(sentence) # 모두 소문자로 변경(단어가 모두 소문자 임)
word.list = str_split(sentence, '\\s+') # 공백 기준으로 단어 생성 -> \\s+ : 공백 정규식, +(1개 이상)
words = unlist(word.list) # unlist() : list를 vector 객체로 구조변경
pos.matches = match(words, posDic) # words의 단어를 posDic에서 matching
neg.matches = match(words, negDic)
pos.matches = !is.na(pos.matches) # NA 제거, 위치(숫자)만 추출
neg.matches = !is.na(neg.matches)
score = sum(pos.matches) - sum(neg.matches) # 긍정 - 부정
return(score)
}, posDic, negDic)      # inner func 종료
scores.df = data.frame(score=scores, text=sentences)
return(scores.df)
}
result<-sentimental(data[,2], posDic.final, negDic.final)
result
names(result) # "score" "text"
dim(result) # 100   2
result$text
result$score # 100 줄 단위로 긍정어/부정어 사전을 적용한 점수 합계
result$color[result$score >=1] <- "blue"
result$color[result$score ==0] <- "green"
result$color[result$score < 0] <- "red"
plot(result$score, col=result$color) # 산포도 색생 적용
barplot(result$score, col=result$color, main ="감성분석 결과화면") # 막대차트
plot(result$score, col=result$color) # 산포도 색생 적용
barplot(result$score, col=result$color, main ="감성분석 결과화면") # 막대차트
table(result$color)
result$score # 100 줄 단위로 긍정어/부정어 사전을 적용한 점수 합계
pie(sentiment_result, main="감성분석 결과",
col=c("blue","red","green"), radius=0.8) # ->  1.2
result$remark[result$score >=1] <- "긍정"
result$remark[result$score ==0] <- "중립"
result$remark[result$score < 0] <- "부정"
sentiment_result<- table(result$remark)
sentiment_result
pie(sentiment_result, main="감성분석 결과",
col=c("blue","red","green"), radius=0.8) # ->  1.2
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre1.8.0_111')
library(tm)
library(KoNLP) # 세종사전. 8만여개정도의 단어가 들어가 있음.
library(wordcloud) # RColorBrewer()함수 제공
judge <- file("C:/NCS/Rwork/Part-II/judge.txt", encoding="UTF-8")
judge <- file("C:/NCS/Rwork/Part-II/judge.txt", encoding="UTF-8")
judge_file <- readLines(judge) # 줄 단위 데이터 생성
judge <- file("C:/NCS/Rwork/Part-II/judge.txt", encoding="UTF-8")
judge_file <- readLines(judge) # 줄 단위 데이터 생성
judge_file
judge <- file("C:/NCS/Rwork/Part-II/judge.txt", encoding="UTF-8")
judge_file <- readLines(judge) # 줄 단위 데이터 생성
judge_file
head(judge_file) # 앞부분 6줄 보기 - 줄 단위 문장 확인
str(judge_file) # chr [1:76]
judge_corpus <- Corpus(VectorSource(judge_file))
judge_corpus
inspect(judge_corpus) # 76개 자료집에 포함된 문자 수 제공
judge_corpus <- Corpus(VectorSource(judge_file))
judge_corpus
inspect(judge_corpus) # 76개 자료집에 포함된 문자 수 제공
judge_file
judge_corpus <- Corpus(VectorSource(judge_file))
judge_corpus
inspect(judge_corpus) # 76개 자료집에 포함된 문자 수 제공
inspect(facebook_corpus) # 76개 자료집에 포함된 문자 수 제공
library(curl)
judge_nouns <- sapply(judge_corpus, exNouns)
judge_nouns[1] # 단어만 추출된 첫 줄 보기
facebook_nouns[1] # 단어만 추출된 첫 줄 보기
myCorputfacebook <- Corpus(VectorSource(judge_nouns))
myCorputjudge <- Corpus(VectorSource(judge_nouns))
myCorputjudge # Content:  documents: 76
myCorputjudge <- tm_map(myCorputjudge, removePunctuation) # 문장부호 제거
myCorputjudge <- tm_map(myCorputjudge, removeNumbers) # 수치 제거
myCorputjudge <- tm_map(myCorputjudge, tolower) # 소문자 변경
myCorputjudge <- tm_map(myCorputjudge, removeWords, stopwords('english')) # 불용어제거
inspect(myCorputjudge[1:5]) # 데이터 전처리 결과 확인
myCorputjudge_txt <- tm_map(myCorputjudge, PlainTextDocument)
myCorputjudge_txt
myCorputjudge_txt <- TermDocumentMatrix(myCorputjudge_txt, control=list(wordLengths=c(2,Inf)))
myCorputjudge_txt
myCorputjudge_txt.df <- as.data.frame(as.matrix(myCorputjudge_txt))
dim(myCorputjudge_txt.df) # [1] 883  76
wordResult <- sort(rowSums(myCorputjudge_txt.df), decreasing=TRUE) # 빈도수로 내림차순 정렬
wordResult[1:10]
myName <- names(wordResult) # 단어 이름 생성 -> 빈도수의 이름
wordcloud(myName, wordResult) # 단어구름 적성
myCorputjudge_txt <- tm_map(myCorputjudge_txt, removePunctuation) # 문장부호 제거
myCorputjudge_txt <- tm_map(myCorputjudge_txt, removeNumbers) # 수치 제거
myCorputjudge_txt <- tm_map(myCorputjudge_txt, tolower) # 소문자 변경
myStopwords = c(stopwords('english'), "하기");
myCorputjudge_txt = tm_map(myCorputjudge_txt, removeWords, myStopwords);
word.df <- data.frame(word=myName, freq=wordResult)
str(word.df) # word, freq 변수
pal <- brewer.pal(12,"Paired") # 12가지 색상 pal <- brewer.pal(9,"Set1") # Set1~ Set3
pal <- brewer.pal(12,"Paired") # 12가지 색상 pal <- brewer.pal(9,"Set1") # Set1~ Set3
windowsFonts(malgun=windowsFont("맑은 고딕"))  #windows
x11( ) # 별도의 창을 띄우는 함수
wordcloud(word.df$word, word.df$freq,
scale=c(5,1), min.freq=3, random.order=F,
rot.per=.1, colors=pal, family="malgun")
judge <- file("C:/NCS/Rwork/Part-II/sermon.txt", encoding="UTF-8")
judge_file <- readLines(judge) # 줄 단위 데이터 생성
judge_file
judge <- file("C:/NCS/Rwork/Part-II/sermon.txt", encoding="UTF-8")
judge_file <- readLines(judge) # 줄 단위 데이터 생성
judge_file
head(judge_file) # 앞부분 6줄 보기 - 줄 단위 문장 확인
judge <- file("C:/NCS/Rwork/Part-II/sermon.txt", encoding="UTF-8")
judge_file <- readLines(judge) # 줄 단위 데이터 생성
judge_file
str(judge_file) # chr [1:76]
judge_corpus <- Corpus(VectorSource(judge_file))
judge_corpus
inspect(judge_corpus) # 76개 자료집에 포함된 문자 수 제공
judge_nouns <- sapply(judge_corpus, exNouns)
judge_nouns[1] # 단어만 추출된 첫 줄 보기
myCorputjudge <- Corpus(VectorSource(judge_nouns))
myCorputjudge # Content:  documents: 76
myCorputjudge <- tm_map(myCorputjudge, removePunctuation) # 문장부호 제거
myCorputjudge <- tm_map(myCorputjudge, removeNumbers) # 수치 제거
myCorputjudge <- tm_map(myCorputjudge, tolower) # 소문자 변경
myCorputjudge <- tm_map(myCorputjudge, removeWords, stopwords('english')) # 불용어제거
inspect(myCorputjudge[1:5]) # 데이터 전처리 결과 확인
myCorputjudge_txt <- tm_map(myCorputjudge, PlainTextDocument)
myCorputjudge_txt
myCorputjudge_txt <- TermDocumentMatrix(myCorputjudge_txt, control=list(wordLengths=c(2,Inf)))
myCorputjudge_txt
myCorputjudge_txt.df <- as.data.frame(as.matrix(myCorputjudge_txt))
dim(myCorputjudge_txt.df) # [1] 883  76
wordResult <- sort(rowSums(myCorputjudge_txt.df), decreasing=TRUE) # 빈도수로 내림차순 정렬
wordResult[1:10]
myName <- names(wordResult) # 단어 이름 생성 -> 빈도수의 이름
wordcloud(myName, wordResult) # 단어구름 적성
myCorputjudge_txt <- tm_map(myCorputjudge_txt, removePunctuation) # 문장부호 제거
myCorputjudge_txt <- tm_map(myCorputjudge_txt, removeNumbers) # 수치 제거
word.df <- data.frame(word=myName, freq=wordResult)
str(word.df) # word, freq 변수
pal <- brewer.pal(12,"Paired") # 12가지 색상 pal <- brewer.pal(9,"Set1") # Set1~ Set3
windowsFonts(malgun=windowsFont("맑은 고딕"))  #windows
x11( ) # 별도의 창을 띄우는 함수
wordcloud(word.df$word, word.df$freq,
scale=c(5,1), min.freq=3, random.order=F,
rot.per=.1, colors=pal, family="malgun")
judge <- file("C:/NCS/Rwork/Part-II/sermon.txt", encoding="UTF-8")
judge_file <- readLines(judge) # 줄 단위 데이터 생성
judge_file
judge_corpus <- Corpus(VectorSource(judge_file))
judge_corpus
exNouns <- function(x) { paste(extractNoun(as.character(x)), collapse=" ")}
judge_nouns <- sapply(judge_corpus, exNouns)
judge_nouns[1] # 단어만 추출된 첫 줄 보기
myCorputjudge <- Corpus(VectorSource(judge_nouns))
myCorputjudge # Content:  documents: 76
myCorputjudge <- tm_map(myCorputjudge, removePunctuation) # 문장부호 제거
myCorputjudge <- tm_map(myCorputjudge, removeNumbers) # 수치 제거
myCorputjudge <- tm_map(myCorputjudge, tolower) # 소문자 변경
myCorputjudge <- tm_map(myCorputjudge, removeWords, stopwords('english')) # 불용어제거
myCorputjudge_txt <- tm_map(myCorputjudge, PlainTextDocument)
myCorputjudge_txt
myCorputjudge_txt <- TermDocumentMatrix(myCorputjudge_txt, control=list(wordLengths=c(2,Inf)))
myCorputjudge_txt
myCorputjudge_txt.df <- as.data.frame(as.matrix(myCorputjudge_txt))
dim(myCorputjudge_txt.df) # [1] 883  76
wordResult <- sort(rowSums(myCorputjudge_txt.df), decreasing=TRUE) # 빈도수로 내림차순 정렬
wordResult[1:10]
myName <- names(wordResult) # 단어 이름 생성 -> 빈도수의 이름
wordcloud(myName, wordResult) # 단어구름 적성
myCorputjudge_txt <- tm_map(myCorputjudge_txt, removePunctuation) # 문장부호 제거
myCorputjudge <- tm_map(myCorputjudge, removePunctuation) # 문장부호 제거
myCorputjudge <- tm_map(myCorputjudge, removeNumbers) # 수치 제거
myCorputjudge <- tm_map(myCorputjudge, tolower) # 소문자 변경
myStopwords = c(stopwords('english'), "하기");
myCorputjudge = tm_map(myCorputjudge, removeWords, myStopwords);
myCorputfacebook_txt <- tm_map(myCorputjudge_txt, PlainTextDocument)
myCorputfacebook_txt
myCorputfacebook_txt <- tm_map(myCorputjudge, PlainTextDocument)
myCorputfacebook_txt
myCorputfacebook_txt <- TermDocumentMatrix(myCorputfacebook_txt, control=list(wordLengths=c(2,Inf)))
myCorputfacebook_txt
k
myTermfacebook.df <- as.data.frame(as.matrix(myCorputfacebook_txt))
dim(myTermfacebook.df) # [1] 880  76
wordResult <- sort(rowSums(myTermfacebook.df), decreasing=TRUE) # 빈도수로 내림차순 정렬
wordResult[1:10]
myName <- names(wordResult) # 단어 이름 추출(빈도수 이름)
wordcloud(myName, wordResult) # 단어구름 시각화
word.df <- data.frame(word=myName, freq=wordResult)
str(word.df) # word, freq 변수
pal <- brewer.pal(12,"Paired") # 12가지 색상 pal <- brewer.pal(9,"Set1") # Set1~ Set3
windowsFonts(malgun=windowsFont("맑은 고딕"))  #windows
x11( ) # 별도의 창을 띄우는 함수
wordcloud(word.df$word, word.df$freq,
scale=c(5,1), min.freq=3, random.order=F,
rot.per=.1, colors=pal, family="malgun")
topWord <- head(sort(wordResult, decreasing=T), 10) # 상위 10개 토픽추출
pie(topWord, col=rainbow(10), radius=1) # 파이 차트-무지개색, 원크기
pct <- round(topWord/sum(topWord)*100, 1) # 백분율
names(topWord)
lab <- paste(names(topWord), "\n", pct, "%")
pie(topWord, main="SNS 빅데이터 관련 토픽분석",
col=rainbow(10), cex=0.8, labels=lab)
judge <- file("C:/NCS/Rwork/Part-II/judge.txt", encoding="UTF-8")
judge_file <- readLines(judge) # 줄 단위 데이터 생성
judge_corpus <- Corpus(VectorSource(judge_file))
judge_corpus
judge_nouns <- sapply(judge_corpus, exNouns)
myCorputjudge <- Corpus(VectorSource(judge_nouns))
myCorputjudge <- tm_map(myCorputjudge, removePunctuation) # 문장부호 제거
myCorputjudge <- tm_map(myCorputjudge, removeNumbers) # 수치 제거
myCorputjudge <- tm_map(myCorputjudge, tolower) # 소문자 변경
myCorputjudge <- tm_map(myCorputjudge, removeWords, stopwords('english')) # 불용어제거
myCorputjudge_txt <- tm_map(myCorputjudge, PlainTextDocument)
myCorputjudge_txt <- TermDocumentMatrix(myCorputjudge_txt, control=list(wordLengths=c(2,Inf)))
myCorputjudge_txt.df <- as.data.frame(as.matrix(myCorputjudge_txt))
wordResult <- sort(rowSums(myCorputjudge_txt.df), decreasing=TRUE) # 빈도수로 내림차순 정렬
myName <- names(wordResult) # 단어 이름 생성 -> 빈도수의 이름
wordcloud(myName, wordResult) # 단어구름 적성
myCorputjudge <- tm_map(myCorputjudge, removePunctuation) # 문장부호 제거
myCorputjudge <- tm_map(myCorputjudge, tolower) # 소문자 변경
myCorputjudge <- tm_map(myCorputjudge, removeNumbers) # 수치 제거
word.df <- data.frame(word=myName, freq=wordResult)
str(word.df) # word, freq 변수
pal <- brewer.pal(12,"Paired") # 12가지 색상 pal <- brewer.pal(9,"Set1") # Set1~ Set3
windowsFonts(malgun=windowsFont("맑은 고딕"))  #windows
x11( ) # 별도의 창을 띄우는 함수
wordcloud(word.df$word, word.df$freq,
scale=c(5,1), min.freq=3, random.order=F,
rot.per=.1, colors=pal, family="malgun")
map3 <- map2 + geom_point(aes(x=lon,y=lat,color=factor(pop$세대수),size=factor(pop$세대수)),data=df)
map2 + geom_point(aes(x=lon,y=lat,color=pop$세대수,size=pop$세대수),data=pop)
pop <- read.csv("C:/NCS/Rwork/Part-II/population201506.csv",header=T)
pop$세대수 <- str_replace_all(pop$세대수, ',','')
pop$세대수 <- as.numeric(pop$세대수)
map1 <- get_map("Jeonju", zoom=7 ,  maptype='roadmap')
map2 <- ggmap(map1)
map2 + geom_point(aes(x=lon,y=lat,color=pop$세대수,size=pop$세대수),data=pop)
library(ggmap)
map1 <- get_map("Jeonju", zoom=7 ,  maptype='roadmap')
map2 <- ggmap(map1)
map2 + geom_point(aes(x=lon,y=lat,color=pop$세대수,size=pop$세대수),data=pop)
map3 <- map2 + geom_point(aes(x=lon,y=lat,color=factor(pop$세대수),size=factor(pop$세대수)),data=df)
map3 + geom_text(data=df, aes(x=lon+0.01, y=lat+0.18,label=region),size=3)
