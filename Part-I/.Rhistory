x2 <- c(2,3,4,1,5) # 홀수 = ceiling([n/2])
median(x2)
sx2 <- sort(x2)
sx2 # 1 2 3 4 5
median(sx2) # 3
med(x1) # 3.5
med <- function(x){
x <- sort(x)
len <- length(x)
if(len%%2==0){
return(result <- (x[len/2]+x[len/2+1])/2)
}else{
return(x[ceiling(len/2)])
}
}
med(x1) # 3.5
x1 <- c(2,3,4,1,5,6) # 짝수 = ([n/2] + [n/2+1]) / 2
x2 <- c(2,3,4,1,5) # 홀수 = ceiling([n/2])
med(x1) # 3.5
med <- function(x){
x <- sort(x)
len <- length(x)
if(len%%2==0){
return(result <- (x[len/2]+x[len/2+1])/2)
}else{
return(x[ceiling(len/2)])
}
}
med(x1) # 3.5
med(x2) # 3
med(x1) # 3.5
med(x2) # 3
x1 <- c(2,3,4,1,5,6) # 짝수 = ([n/2] + [n/2+1]) / 2
med(x1) # 3.5
med <- function(x){
x <- sort(x)
len <- length(x)
if(len%%2==0){
return((x[len/2]+x[len/2+1])/2)
}else{
return(x[ceiling(len/2)])
}
}
med(x1) # 3.5
med(x2) # 3
rnorm(20, mean = 0, sd = 1) # 표준정규분포를 따르는 20개 데이터 생성
r2 <- runif(n, min=0, max=1) # 0 < r2 < 1
r2
runif(20, min=0, max=100) # 0~100사이의 20개 난수 생성
?rnorm
n <- 1000
r <- rnorm(n, mean = 0, sd = 1)
r
hist(r) # 대칭성
n <- 100000
r <- rnorm(n, mean = 0, sd = 1)
hist(r) # 대칭성
r
hist(r) # 대칭성
r2 <- runif(n, min=0, max=1) # 0 < r2 < 1
r2
n <- 1000
r2 <- runif(n, min=0, max=1) # 0 < r2 < 1
r2
r <- rnorm(n, mean = 0, sd = 1)
r
r2 <- runif(n, min=-1, max=1) # 0 < r2 < 1
r2
hist(r2)
r2 <- runif(n, min=0, max=1) # 0 < r2 < 1
r2
hist(r2)
r2 <- runif(n, min=0, max=1) # 0 < r2 < 1
r2
hist(r2)
r2 <- runif(n, min=0, max=1) # 0 < r2 < 1
hist(r2)
n <- 100000
r2 <- runif(n, min=0, max=1) # 0 < r2 < 1
hist(r2)
set.seed(123) # seed()함수를 실행하면 같은 값으로 난수 생성
n <- 10
r3 <- rbinom(n, 1, 0.5) # 1개 size에서 1/2(0.5)확률로 난수 정수 생성
r3 #  0 1 0 1 1 0 1 1 1 0
n <- 10
r3 <- rbinom(n, 1, 0.5) # 1개 size에서 1/2(0.5)확률로 난수 정수 생성
r3 <- rbinom(n, 1, 0.25) # 1개 size에서 1/4(0.25)확률로 난수 정수 생성
r3 #  1 0 0 1 0 0 0 0 0 0
r3 <- rbinom(n, 3, 0.5) # 3개 size에서 1/2(0.5)0.5확률로 난수 정수 생성
r3 #  3 1 2 2 0 3 1 0 1 3
n <- 10
r3 <- rbinom(n, 1, 0.5) # 1개 size에서 1/2(0.5)확률로 난수 정수 생성
r3 #  0 1 0 1 1 0 1 1 1 0
r3 <- rbinom(n, 1, 0.5) # 1개 size에서 1/2(0.5)확률로 난수 정수 생성
r3 #  0 1 0 1 1 0 1 1 1 0
set.seed(123) # seed()함수를 실행하면 같은 값으로 난수 생성
n <- 10
r3 <- rbinom(n, 1, 0.5) # 1개 size에서 1/2(0.5)확률로 난수 정수 생성
r3 <- rbinom(n, 1, 0.25) # 1개 size에서 1/4(0.25)확률로 난수 정수 생성
r3 #  1 0 0 1 0 0 0 0 0 0
r3 <- rbinom(n, 1, 1) # 1개 size에서 1/2(0.5)확률로 난수 정수 생성
r3 #  0 1 0 1 1 0 1 1 1 0
r3 <- rbinom(n, 1, 0.9) # 1개 size에서 1/2(0.5)확률로 난수 정수 생성
r3 #  0 1 0 1 1 0 1 1 1 0
r3 <- rbinom(n, 1, 0.8) # 1개 size에서 1/2(0.5)확률로 난수 정수 생성
r3 #  0 1 0 1 1 0 1 1 1 0
r3 <- rbinom(n, 1, 0.8) # 1개 size에서 1/2(0.5)확률로 난수 정수 생성
r3 #  0 1 0 1 1 0 1 1 1 0
r3 <- rbinom(n, 1, 0.25) # 1개 size에서 1/4(0.25)확률로 난수 정수 생성
r3 #  1 0 0 1 0 0 0 0 0 0
r3 <- rbinom(n, 3, 0.5) # 3개 size에서 1/2(0.5)0.5확률로 난수 정수 생성
r3 #  3 1 2 2 0 3 1 0 1 3
r3 <- rbinom(n, 3, 0.5) # 3개 size에서 1/2(0.5)0.5확률로 난수 정수 생성
r3 #  3 1 2 2 0 3 1 0 1 3
r3 <- rbinom(n, 3, 0.5) # 3개 size에서 1/2(0.5)0.5확률로 난수 정수 생성
r3 #  3 1 2 2 0 3 1 0 1 3
set.seed(234)
r3 <- rbinom(n, 1, 0.5) # 1개 size에서 1/2(0.5)확률로 난수 정수 생성
r3 #  0 1 0 1 1 0 1 1 1 0
rnorm(5, 0, 1)
set.seed(123)   # 임의의 정수만 넣어주면
rnorm(5, 0, 1)
set.seed(123)   # 임의의 정수만 넣어주면
rnorm(5, 0, 1)
set.seed(123)   # 임의의 정수만 넣어주면
rnorm(5, 0, 1)
rnorm(5, 0, 1)
rnorm(5, 0, 1)
rnorm(5, 0, 1)
set.seed(123)   # 임의의 정수만 넣어주면
rnorm(5, 0, 1)
set.seed(123)   # 임의의 정수만 넣어주면
rnorm(5, 0, 1)
set.seed(345)
rnorm(5, 0, 1)
set.seed(345)
rnorm(5, 0, 1)
sample(10, 5)
sample(1:10, 5)
sample(2:10, 5)
sample(10, 10)
sample(2:10, 10)
sample(2:10, 9)
sample(20:100, 5)
no <- c(1:5)
score <- c(10,30,40,10,50)
df <- data.frame(no,score)
df
nrow(df)
sample(nrow(df), nrow(df)*0.7)
sample(nrow(df), nrow(df)*0.7)  # 70%만 사용하겠다.
sample(nrow(df), nrow(df)*0.7)  # 70%만 사용하겠다.
idx <- sample(nrow(df), nrow(df)*0.7)  # 70%만 사용하겠다.
train <- df[idx,]
train
test <- df[-idx,]
test
train   # 70% - 학습데이터
train <- df[sort(idx),]
train   # 70% - 학습데이터
train <- df[idx,]
train   # 70% - 학습데이터
install.packages("RSADBE")
library(RSADBE)
data
data()
Bug_Metrics_Software
RSADBE::Bug_Metrics_Software
data()
data.Bug_Metrics_Software
read(Bug_Metrics_Software)
read.table(Bug_Metrics_Software)
data()
data(package = rsadbe.packages(all.available = TRUE))
data(package = RSADBE.packages(all.available = TRUE))
data(Bug_Metrics_Software)
Bug_Metrics_Software
class(Bug_Metrics_Software)
mode()
mode(Bug_Metrics_Software)
Bug_Metrics_Software[,,2]
Bug_Metrics_Software
class(Bug_Metrics_Software)
Bug_Metrics_Software[1]
Bug_Metrics_Software[,1]
Bug_Metrics_Software[1,]
Bug_Metrics_Software[[1]]
Bug_Metrics_Software[1,1
Bug_Metrics_Software[1,1]
Bug_Metrics_Software[1,1]
Bug_Metrics_Software[,,after]
Bug_Metrics_Software[,,After]
Bug_Metrics_Software
()
data()
str(Bug_Metrics_Software)
head(Bug_Metrics_Software)
Bug_Metrics_Software
str(Bug_Metrics_Software)
Bug_Metrics_Software[1:5,1:5,1:2]
Bug_Metrics_Software[1,,]
Bug_Metrics_Software
Bug_Metrics_Software[1,,]
Bug_Metrics_Software[1,1,1]
Bug_Metrics_Software[,,1]
Bug_Metrics_Software[,,1]
Bug_Metrics_Software[1,,]
Bug_Metrics_Software[,1,]
Bug_Metrics_Software[,1,]
Bug_Metrics_Software[,,1]
Bug_Metrics_Software[,,2]
Bug_Metrics_Software[1,,]
Bug_Metrics_Software[,,2]
df <- Bug_Metrics_Software[,,2]
df
class(df)
mode(df)
colMeans(df[1:5])
colMeans(df[,1:5])
df
rowSums(df[1:5,])
summary(df)
summary(df[,1:5])
colMeans(df[,1:5])
dff <- df[,1:5]
summary(dff)
as.matrix(df)
df <- as.matrix(df)
summary(df)
df <- as.table(df)
summary(df)
summary(test) # 요약통계량
class(test)
df <- as.data.frame(df)
summary(df)
df
df <- Bug_Metrics_Software[,,2]
df
class(df)
df <- as.data.frame(df)
class(df)
df
df <- Bug_Metrics_Software[,,2]
class(df)
str(df)
ggmap(ggmap, extent = "panel", base_layer, maprange = FALSE,
legend = "right", padding = 0.02, darken = c(0, "black"), ...)
install.packages("ggplot2")
library(stringr)
library(ggplot2)
library(ggmap)
install.packages("ggmap") # ‘ggmap’와 ‘ggplot2’(우선 설치) 관련 패키지
library(ggmap)
map <- get_map(location ="seoul", zoom=10, maptype='roadmap', source = "google")
maps <- get_map(location ="seoul", zoom=10, maptype='roadmap', source = "naver")
ggmap(map, size=c(600,400), extent='device')
map <- get_map(location ="seoul", zoom=30, maptype='roadmap', source = "google")
ggmap(map, size=c(600,400), extent='device')
map <- get_map(location ="seoul", zoom=20, maptype='roadmap', source = "google")
ggmap(map, size=c(600,400), extent='device')
map <- get_map(location ="seoul", zoom=15, maptype='roadmap', source = "google")
ggmap(map, size=c(600,400), extent='device')
map <- get_map(location ="seoul", zoom=12, maptype='roadmap', source = "google")
ggmap(map, size=c(600,400), extent='device')
map <- get_map(location ="seoul", zoom=10, maptype='roadmap', source = "google")
gc <- geocode("seoul")
gc
gc <- geocode("waco, texas") # texas주 중부 도시
center <- as.numeric(gc)
center # 위도,경도
gc <- geocode("seoul")
gc
center <- as.numeric(gc)
center # 위도,경도  #[1] -97.14667  31.54933
map <- get_googlemap(center = center, language="ko-KR", color = "bw", scale = 2 )
ggmap(map, extent = 'device')
df <- round(data.frame( x = jitter(rep(-95.36, 25), amount = .3),
y = jitter(rep(29.76, 25), amount = .3) ), digits = 2)
df
map <- get_googlemap('houston', markers = df, path = df, scale = 2)
ggmap(map, extent = 'device')
map <- get_map(location ="london", zoom=14, maptype='roadmap', scale=2)
ggmap(map, size=c(600,600), extent='device')
map <- get_map(location ="seoul", zoom=8, maptype='watercolor', scale=2)
ggmap(map, size=c(600,600), extent='device')
map <- get_map(location ="seoul", zoom=14, scale=2)
ggmap(map, size=c(600,600), extent='device')
map <- get_map(location ="seoul", zoom=8, scale=2)
ggmap(map, size=c(600,600), extent='device')
map <- get_map(location ="london", zoom=14, maptype='roadmap', scale=2)
map <- get_map(location = "texas", zoom = 6, source = "stamen")
ggmap(map, size=c(600,600), extent='device')
map <- get_map(location ="seoul", source = "osm", zoom=8, maptype='watercolor')
ggmap(map, size=c(600,600), extent='device')
university <- read.csv("C:/NCS/Rwork/Part-III/university.csv",header=T)
university <- read.csv("C:/NCS/Rwork/Part-II/university.csv",header=T)
university # # 학교명","LAT","LON"
kor <- get_map("seoul", zoom=11, maptype = "watercolor")#roadmap
ggmap(kor)
ggmap(kor)+geom_point(data=university, aes(x=LON, y=LAT,color=factor(학교명)),size=3)
kor.map <- ggmap(kor)+geom_point(data=university, aes(x=LON, y=LAT,color=factor(학교명)),size=3)
geom_text(data=university, aes(x=LON+0.01, y=LAT+0.01,label=학교명),size=5)
kor.map + geom_text(data=university, aes(x=LON+0.01, y=LAT+0.01,label=학교명),size=5)
ggsave("C:/Rwork/output/university1.png",width=10.24,height=7.68)
ggsave("C:/NCS/Rwork/output/university1.png",width=10.24,height=7.68)
ggsave("C:/NCS/Rwork/output/university2.png",dpi=1000) # 9.21 x 7.68 in image
pop <- read.csv("C:/NCS/Rwork/Part-II/population201506.csv",header=T)
pop
str(pop)
region <- pop$지역명
lon <- pop$LON # 위도
lat <- pop$LAT # 경도
region <- pop$지역명
lon <- pop$LON # 위도
lat <- pop$LAT # 경도
region <- pop$지역명
lon <- pop$LON # 위도
region <- pop$지역명
lon <- pop$LON # 위도
lat <- pop$LAT # 경도
house <- pop$세대수
# 위도,경도,세대수 이용 데이터프레임 생성
df <- data.frame(region, lon,lat,house)
house <- pop$세대수
# 위도,경도,세대수 이용 데이터프레임 생성
df <- data.frame(region, lon,lat,house)
df
map1 <- get_map("daegu", zoom=7 ,  maptype='watercolor')
map2 <- ggmap(map1)
map2
map2 <- ggmap(map1)
map2
map2 + geom_point(aes(x=lon,y=lat,colour=house,size=house),data=df)
map3 <- map2 + geom_point(aes(x=lon,y=lat,colour=house,size=house),data=df)
map2 + geom_point(aes(x=lon,y=lat,color=house,size=house),data=df)
map3 <- map2 + geom_point(aes(x=lon,y=lat,color=house,size=house),data=df)
map3 + geom_text(data=df, aes(x=lon+0.01, y=lat+0.18,label=region),size=3)
ggsave("C:/NCS/Rwork/output/population201506.png",scale=1,width=10.24,height=7.68)
map1 <- get_map("daegu", zoom=7 ,  maptype='terrain')
map2 <- ggmap(map1)
map3 <- map2 + geom_point(aes(x=lon,y=lat,colour=house,size=house),data=df)
map3 + geom_text(data=df, aes(x=lon+0.01, y=lat+0.18,label=region),size=3)
map3 <- map2 + geom_point(aes(x=lon,y=lat,color=house,size=house),data=df)
map3 + geom_text(data=df, aes(x=lon+0.01, y=lat+0.18,label=region),size=3)
university <- read.csv("C:/NCS/Rwork/Part-II/university.csv",header=T)
university # 구청명  LAT(경도)    LON(위도)
df <- round(data.frame( x = jitter(rep(-95.36, 25), amount = .3),
y = jitter(rep(29.76, 25), amount = .3) ), digits = 2)
df
map <- get_googlemap('houston', markers = df, path = df, scale = 2)
ggmap(map, extent = 'device')
str(university)
df <- data.frame(university$LAT, university$LON)
map <- map_googlemap('Seoul', marker=df, path = df, scale=2)
map <- get_googlemap('Seoul', marker=df, path = df, scale=2)
ggmap(map, extent = 'device')
df <- round(data.frame(university$LAT, university$LON), digits=2)
map <- get_googlemap('Seoul', marker=df, path = df, scale=2)
map <- get_googlemap('Seoul', markers=df, path = df, scale=2)
ggmap(map, extent = 'device')
df <- round(data.frame(university$LON, university$LAT), digits=2)
map <- get_googlemap('Seoul', markers=df, path = df, scale=2)
ggmap(map, extent = 'device')
pop <- read.csv("C:/NCS/Rwork/Part-II/population201506.csv",header=T)
pop
map1 <- get_map("Jeonju", zoom=7 ,  maptype='roadmap')
map2 <- ggmap(map1)
map2
pop
map2 + geom_point(aes(x=lon,y=lat,color=house,size=house),data=pop)
map3 <- map2 + geom_point(aes(x=lon,y=lat,color=house,size=house),data=df)
map3 + geom_text(data=df, aes(x=lon+0.01, y=lat+0.18,label=region),size=3)
str(pop)
pop
str_replace_all(pop$세대수, ',','')
pop$세대수 <- str_replace_all(pop$세대수, ',','')
as.numeric(pop$세대수)
pop$세대수 <- as.numeric(pop$세대수)
str(pop)
map2 + geom_point(aes(x=lon,y=lat,color=pop$세대수,size=pop$세대수),data=pop)
map3 <- map2 + geom_point(aes(x=lon,y=lat,color=pop$세대수,size=pop$세대수),data=df)
map3 + geom_text(data=df, aes(x=lon+0.01, y=lat+0.18,label=region),size=3)
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre1.8.0_111')
library(rJava) # 로딩
install.packages(c("KoNLP", "tm", "wordcloud"))
library(KoNLP)
library(tm)
library(wordcloud) # RColorBrewer()함수 제공
facebook <- file("C:/NCS/Rwork/Part-II/facebook_bigdata.txt", encoding="UTF-8")
str(facebook)
facebook_data <- readLines(facebook) # 줄 단위 데이터 생성
head(facebook_data) # 앞부분 6줄 보기 - 줄 단위 문장 확인
str(facebook_data) # chr [1:76]
facebook_corpus <- Corpus(VectorSource(facebook_data))
facebook_corpus
inspect(facebook_corpus) # 76개 자료집에 포함된 문자 수 제공
useSejongDic() # 세종 사전 불러오기
library(KoNLP) # 세종사전. 8만여개정도의 단어가 들어가 있음.
useSejongDic() # 세종 사전 불러오기
install.packages(c("KoNLP", "tm", "wordcloud"))
install.packages(c("KoNLP", "tm", "wordcloud"))
install.packages(c("KoNLP", "tm", "wordcloud"))
install.packages(c("KoNLP", "tm", "wordcloud"))
library(KoNLP) # 세종사전. 8만여개정도의 단어가 들어가 있음.
library(tm)     # 영문 텍스트 마이닝 관련 라이브러리
library(wordcloud) # RColorBrewer()함수 제공
facebook <- file("C:/NCS/Rwork/Part-II/facebook_bigdata.txt", encoding="UTF-8")
facebook_data <- readLines(facebook) # 줄 단위 데이터 생성
facebook_corpus <- Corpus(VectorSource(facebook_data))
useSejongDic() # 세종 사전 불러오기
useSejongDic() # 세종 사전 불러오기
extractNoun('우리나라 대한민국 나는 홍길동 입니다.')
exNouns <- function(x) { paste(extractNoun(as.character(x)), collapse=" ")}
facebook_nouns <- sapply(facebook_corpus, exNouns)
facebook_nouns[1] # 단어만 추출된 첫 줄 보기
facebook_corpus
inspect(facebook_corpus) # 76개 자료집에 포함된 문자 수 제공
myCorputfacebook <- Corpus(VectorSource(facebook_nouns))
myCorputfacebook # Content:  documents: 76
myCorputfacebook <- tm_map(myCorputfacebook, removePunctuation) # 문장부호 제거
myCorputfacebook <- tm_map(myCorputfacebook, removeNumbers) # 수치 제거
myCorputfacebook <- tm_map(myCorputfacebook, tolower) # 소문자 변경
myCorputfacebook <-tm_map(myCorputfacebook, removeWords, stopwords('english')) # 불용어제거
stopwords('english')
inspect(myCorputfacebook[1:5]) # 데이터 전처리 결과 확인
inspect(myCorputfacebook[1:5]) # 데이터 전처리 결과 확인
useSejongDic() # 세종 사전 불러오기
useSejongDic()
install.packages('curl')
library(curl)
useSejongDic() # 세종 사전 불러오기
mergeUserDic(data.frame(c("R 프로그래밍","페이스북","소셜네트워크"), c("ncn")))
inspect(myCorputfacebook[1:5]) # 데이터 전처리 결과 확인
myCorputfacebook_txt <- tm_map(myCorputfacebook, PlainTextDocument)
myCorputfacebook_txt
myCorputfacebook_txt <- TermDocumentMatrix(myCorputfacebook_txt, control=list(wordLengths=c(2,Inf)))
myCorputfacebook_txt
myCorputfacebook_txt <- TermDocumentMatrix(myCorputfacebook_txt, control=list(wordLengths=c(2,Inf)))
myCorputfacebook_txt
inspect(myCorputfacebook_txt)
myCorputfacebook_txt <- TermDocumentMatrix(myCorputfacebook_txt, control=list(wordLengths=c(2,Inf)))
install.packages("SnowballC")
library(SnowballC)
myCorputfacebook_txt <- tm_map(myCorputfacebook, PlainTextDocument)
inspect(myCorputfacebook_txt)
myCorputfacebook_txt
myCorputfacebook_txt <- TermDocumentMatrix(myCorputfacebook_txt, control=list(wordLengths=c(2,Inf)))
myCorputfacebook_txt
myCorputfacebook_txt <- TermDocumentMatrix(myCorputfacebook_txt, control=list(wordLengths=c(2,Inf)))
facebook_data <- readLines(facebook, ok = TRUE, warn = FALSE, encoding = "utf-8") # 줄 단위 데이터 생성
convFacebook <- iconv(facebook_data, to = "utf-8")
head(convFacebook) # 앞부분 6줄 보기 - 줄 단위 문장 확인
facebook_data <- readLines(facebook, ok = TRUE, warn = FALSE, encoding = "utf-8") # 줄 단위 데이터 생성
convFace <- iconv(facebook_data, to = "utf-8")
tweets <- (convFace[!is.na(convFace)])
head(tweets) # 앞부분 6줄 보기 - 줄 단위 문장 확인
str(tweets) # chr [1:76]
facebook <- file("C:/NCS/Rwork/Part-II/facebook_bigdata.txt", encoding="UTF-8")
facebook_data <- readLines(facebook) # 줄 단위 데이터 생성
facebook_corpus <- Corpus(VectorSource(facebook_data))
facebook_corpus
inspect(facebook_corpus) # 76개 자료집에 포함된 문자 수 제공
mergeUserDic(data.frame(c("R 프로그래밍","페이스북","소셜네트워크"), c("ncn")))
exNouns <- function(x) { paste(extractNoun(as.character(x)), collapse=" ")}
facebook_nouns <- sapply(facebook_corpus, exNouns)
facebook_nouns[1] # 단어만 추출된 첫 줄 보기
facebook_nouns[1] # 단어만 추출된 첫 줄 보기
myCorputfacebook <- Corpus(VectorSource(facebook_nouns))
myCorputfacebook # Content:  documents: 76
myCorputfacebook # Content:  documents: 76
myCorputfacebook <- tm_map(myCorputfacebook, removePunctuation) # 문장부호 제거
myCorputfacebook <- tm_map(myCorputfacebook, removeNumbers) # 수치 제거
myCorputfacebook <- tm_map(myCorputfacebook, tolower) # 소문자 변경
myCorputfacebook <- tm_map(myCorputfacebook, removeWords, stopwords('english')) # 불용어제거
inspect(myCorputfacebook[1:5]) # 데이터 전처리 결과 확인
myCorputfacebook_txt <- tm_map(myCorputfacebook, PlainTextDocument)
myCorputfacebook_txt
myCorputfacebook_txt <- TermDocumentMatrix(myCorputfacebook_txt$content, control=list(wordLengths=c(2,Inf)))
myCorputfacebook_txt
myCorputfacebook_txt <- TermDocumentMatrix(myCorputfacebook_txt$meta, control=list(wordLengths=c(2,Inf)))
myCorputfacebook_txt <- TermDocumentMatrix(myCorputfacebook_txt, control=list(wordLengths=c(2,Inf)))
facebook_data
facebook_corpus <- Corpus(VectorSource(facebook_data))
facebook_corpus
inspect(facebook_corpus) # 76개 자료집에 포함된 문자 수 제공
facebook_corpus
str(facebook_data) # chr [1:76]
facebook_data
facebook[1]
facebook[[1]]
facebook_data[1]
facebook_corpus[1]
facebook_data
facebook_corpus <- Corpus(VectorSource(facebook_data))
facebook_corpus
inspect(facebook_corpus) # 76개 자료집에 포함된 문자 수 제공
facebook_corpus
exNouns <- function(x) { paste(extractNoun(as.character(x)), collapse=" ")}
facebook_nouns <- sapply(facebook_corpus, exNouns)
facebook_nouns[1] # 단어만 추출된 첫 줄 보기
myCorputfacebook_txt <- tm_map(myCorputfacebook, PlainTextDocument)
myCorputfacebook_txt
myCorputfacebook_txt <- TermDocumentMatrix(myCorputfacebook_txt, control=list(wordLengths=c(2,Inf)))
myCorputfacebook_txt
library(SnowballC)
myCorputfacebook_txt <- tm_map(myCorputfacebook,stemDocument) #평서문으로 변경
myCorputfacebook_txt <- PlainTextDocument(myCorputfacebook)
myCorputfacebook_txt
myCorputfacebook_txt <- TermDocumentMatrix(myCorputfacebook_txt, control=list(wordLengths=c(2,Inf)))
myCorputfacebook_txt
install.packages("http://cran.r-project.org/bin/windows/contrib/3.0/tm_0.5-10.zip",repos=NULL)
library(tm)
